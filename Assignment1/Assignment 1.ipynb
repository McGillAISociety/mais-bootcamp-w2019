{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This assignment provides a brief introduction to the machine learning process that we will cover in depth in this course. We want to show a simple workflow of how you can use the sklearn library for machine learning. We will be using the MNIST handwritten digits dataset to dive into the machine learning process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let us import some libraries.\n",
    "\n",
    "1) numpy - http://www.numpy.org/ - A library for dealing with N-dimensional arrays in python. Numpy provides efficient implementations of common numerical computations used in linear algebra.\n",
    "\n",
    "2) sklearn - https://scikit-learn.org/stable/ - A data analysis library that provides implementations of many machine learning algorithms (and much more!).\n",
    "\n",
    "3) matplotlib - https://matplotlib.org/ - A python 2D plotting library used for visualizations (data, charts, etc.)\n",
    "\n",
    "These libraries (and many more) are often used together and built on top of each other. For example, sklearn depends on numpy and uses numpy arrays under the hood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from scipy import io\n",
    "from sklearn.datasets import fetch_mldata\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler,StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LinearRegression, Ridge, LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this homework assignment, we will be using the MNIST dataset. The MNIST data is a collection of black and white 28x28 images, each picturing a handwritten digit. These were collected from digits people write at the post office, and now this dataset is a standard benchmark to evaluate models against used in the machine learning community. We have provided the .mat file in the assignment repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'__header__': b'MATLAB 5.0 MAT-file Platform: posix, Created on: Sun Mar 30 03:19:02 2014', '__version__': '1.0', '__globals__': [], 'mldata_descr_ordering': array([[array(['label'],\n",
      "      dtype='<U5'),\n",
      "        array(['data'],\n",
      "      dtype='<U4')]], dtype=object), 'data': array([[0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       ..., \n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0]], dtype=uint8), 'label': array([[ 0.,  0.,  0., ...,  9.,  9.,  9.]])}\n"
     ]
    }
   ],
   "source": [
    "mnist = io.loadmat('mnist-original.mat', struct_as_record=True) #Loads Matlab File. Struct_as_record (True = Numpy Record array) else numpy arrays with objects\n",
    "print(mnist) #What are we reading it to? Its numpy arrays?\n",
    "X = mnist['data'].astype('float64').T # Transpose the matrix because we want each row to be an example\n",
    "y = mnist['label'].astype('int64').T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us first explore this data a little bit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(70000, 784) (70000, 1)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The X matrix here contains all the digit pictures. The data is (n_samples x n_features), meaning this data contains 70000 pictures, each with 784 features (the 28x28 image is flattened into a single row). The y vector contains the label for each digit, so we know which digit (or class - class means category) is in each picture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try and visualize this data a bit. Change around the index variable to explore more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28, 28)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x27052572240>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAEICAYAAACQ6CLfAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2705146b9b0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "index = 15000 #15000, 28999, 67345\n",
    "image = X[index].reshape((28, 28)) #Tuple for reshape size. Ordering Default is C. \n",
    "print(image.shape) # Shows the pixel values at each cell in the matrix. Ranges from 0-255\n",
    "plt.title('Label is ' + str(y[index]))\n",
    "plt.imshow(image, cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that each pixel value ranges from 0-255. When we train our models, a good practice is to *standardize* the data so different features can be compared more equally (it also speeds up computation). Here we will use a simple standardization, squeezing all values into the [0, 1] interval range. This kind of standardization is called min-max normalization. For other methods, see https://en.wikipedia.org/wiki/Feature_scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1.05 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "MMSc = MinMaxScaler()\n",
    "X=MMSc.fit_transform(X) #Fits Compute the Max and the Min,then transform it with the scaling\n",
    "#X = X / 255 # Shorthand for dividing all values in the X matrix by 255. Numpy provides lots of shortcuts like this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we train our model, we want it to have the lowest error. Error presents itself in 2 ways: bias (how close our model is to the ideal model), and variance (how much our model varies with different datasets). If we train our model on a chunk of data, and then test our model on that same data, we will only witness the first type of error - bias. However, if we test on new, unseen data, that will reflect both bias and variance. This is the reasoning behind cross validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we want to have 2 datasets, train and test, each used for the named purpose exclusively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 179 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(52500, 784) (17500, 784)\n",
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print(X_train.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will walk you through applying various models to try and achieve the lowest error rate on this data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each of our labels is a number from 0-9. That is, our labels are categorical. If we simply did regression on this data, the labels would imply some sort of ordering and distance between the classes (imagine we were classing colors instead, there is no notion of distance or order i.e. 8 is not \"more\" of a digit than 7). We can fix this issue by one-hot encoding our labels. So, instead of each label being a simple digit, each label is a vector of 10 entries. 9 of those entries are zero, and only 1 entry is equal to one, corresponding to the index of the digit. This way, the distance between the labels are the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 18 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', module='sklearn')\n",
    "\n",
    "#Use One Hot Encoder for Cause Nominal Label.\n",
    "enc = OneHotEncoder(sparse=False)\n",
    "y_hot = enc.fit_transform(y.reshape(-1, 1))\n",
    "y_train_hot = enc.transform(y_train.reshape(-1, 1))\n",
    "y_hot.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember how the first sample is the digit zero? Let's now look at the new label at that index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hot[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 3 steps to build your model: \n",
    "\n",
    "1) Create the model\n",
    "\n",
    "2) Train the model\n",
    "\n",
    "3) Use your model to make predictions\n",
    "\n",
    "In the sklearn API, this is made very clear. First you instantiate the model (constructor), then you call train it with the `fit` method, then you can make predictions on new data with the `test` method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's do a basic linear regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 3.37 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Instantiate the model\n",
    "linear = LinearRegression()\n",
    "# Fit the model (train)\n",
    "linear.fit(X_train, y_train_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train acc:  0.858819047619\n",
      "test acc:  0.851028571429\n",
      "(10, 784)\n",
      "Wall time: 109 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# use trained model to predict both train and test sets\n",
    "y_train_pred = linear.predict(X_train)\n",
    "y_test_pred = linear.predict(X_test)\n",
    "\n",
    "# print accuracies\n",
    "print('train acc: ', accuracy_score(y_train_pred.argmax(axis=1), y_train))\n",
    "print('test acc: ', accuracy_score(y_test_pred.argmax(axis=1), y_test))\n",
    "print(linear.coef_.shape)\n",
    "#Why is it 10 rows?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note on interpretability: you can view the weights of your model with `linear.coef_`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ridge Regression (L2) is one method of preventing a common problem in machine learning called \"overfitting\". Remember when we split our examples into training and test sets? Overfitting occurs when the model performs well on the training set but not on the test set. This means that the model does not generalize well to previously unseen examples.\n",
    "\n",
    "Let us try Ridge Regression and see if we get better results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train acc:  0.858857142857\n",
      "test acc:  0.851542857143\n",
      "Wall time: 849 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "ridge = Ridge(alpha=1)\n",
    "ridge.fit(X_train, y_train_hot)\n",
    "print('train acc: ', accuracy_score(ridge.predict(X_train).argmax(axis=1), y_train))\n",
    "print('test acc: ', accuracy_score(ridge.predict(X_test).argmax(axis=1), y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The alpha controls how much to penalize the weights. Play around with it to see if you can improve the test accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you have seen how to use some basic models to fit and evaluate your data. You will now walk through working with more models. Fill in code where needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now do logistic regression. From now on, the models will automatically one-hot the labels (so we don't need to worry about it)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train acc:  0.918457142857\n",
      "test acc:  0.916228571429\n",
      "Wall time: 10.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "logreg = LogisticRegression(C=0.01, multi_class='multinomial', solver='saga', tol=0.1)\n",
    "logreg.fit(X_train, y_train)\n",
    "print('train acc: ', accuracy_score(logreg.predict(X_train), y_train))\n",
    "print('test acc: ', accuracy_score(logreg.predict(X_test), y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our accuracy has jumped ~5%! Why is this? Logistic Regression is a more complex model - instead of computing raw scores as in linear regression, it does one extra step and squashes values between 0 and 1. This means our model now optimizes over *probabilities* instead of raw scores. This makes sense since our vectors are 1-hot encoded."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The C hyperparameter controls inverse regularization strength (inverse for this model only). Reguralization is important to make sure our model doesn't overfit (perform much better on train data than test data). Play around with the C parameter to try and get better results! You should be able to hit 92%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1) Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision Trees are a completely different type of classifier. They essentially break up the possible space by repeatedly \"splitting\" on features to keep narrowing down the possibilities. Decision Trees are normally individually very week, so we typically average them together in bunches called Random Forest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you have seen many examples for how to construct, fit, and evaluate a model. Now do the same for Random Forest using the [documentation here](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html). You should be able to create one easily without needing to specify any constructor parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:5: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  \"\"\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "0.967885714286\n",
      "Wall time: 53.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "warnings.filterwarnings('ignore', module='sklearn')\n",
    "## YOUR CODE HERE - call the constructor\n",
    "RFC = RandomForestClassifier(n_estimators=161 ,min_samples_split=3);\n",
    "## YOUR CODE HERE - fit the rf model (just like logistic regression)\n",
    "RFC.fit(X_train, y_train)\n",
    "## YOUR CODE HERE - print training accuracy\n",
    "print(accuracy_score(RFC.predict(X_train), y_train))\n",
    "## YOUR CODE HERE - print test accuracy\n",
    "print(accuracy_score(RFC.predict(X_test), y_test))\n",
    "\n",
    "#Why the more you decrease your treshold the higher the accuracy? Less overfitting? Using all feature overfits. so...\n",
    "\n",
    "#40n-est,min-sample-2, train = 1.0, test = 0.964\n",
    "#33, 3, 0.9646\n",
    "#120, 8, 0.9664\n",
    "#120, 6, 0.967257\n",
    "#120, 5, 0.967371428571\n",
    "#120, 4, 0.968114285714\n",
    "#120, 3, 0.968685714286\n",
    "#160, 3, 0.969771428571"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That train accuracy is amazing, let's see if we can boost up the test accuracy a bit (since that's what really counts). Try and play around with the hyperparameters to see if you can edge out more accuracy (look at the documentation for parameters in the constructor). Focus on `n_estimators`, `min_samples_split`, `max_features`. You should be able to hit ~97%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A support vector classifier is another completely different type of classifier. It tries to find the best separating hyperplane through your data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The SVC will toast our laptops unless we reduce the data dimensionality. Let's keep 80% of the variation, and get rid of the rest. (This will cause a slight drop in performance, but not by much)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 4.59 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "pca = PCA(n_components=0.8, whiten=True)\n",
    "X_train_pca = pca.fit_transform(X_train)\n",
    "X_test_pca = pca.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! Now let's take a look at what that actually did."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0 ns\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(52500, 43)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "X_train_pca.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember, before we had 784 (28x28) features! However, PCA found just 43 basis features that explain 80% of the data. So, we went to just 5% of the original input space, but we still retained 80% of the information! Nice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This [blog post](http://colah.github.io/posts/2014-10-Visualizing-MNIST/) explains dimensionality reduction with MNIST quite well. It's a short read (<10 mins), and it contains some pretty cool visualizations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's train our first SVC. The LinearSVC can only find a linear decision boundary (the hyperplane)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train acc:  0.89140952381\n",
      "test acc:  0.894857142857\n",
      "Wall time: 2.15 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "lsvc = LinearSVC(dual=False, tol=0.01)\n",
    "lsvc.fit(X_train_pca, y_train)\n",
    "print('train acc: ', accuracy_score(lsvc.predict(X_train_pca), y_train))\n",
    "print('test acc: ', accuracy_score(lsvc.predict(X_test_pca), y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVMs are really interesting because they have something called the *dual formulation*, in which the computation is expressed as training point inner products. This means that data can be lifted into higher dimensions easily with this \"kernel trick\". Data that is not linearly separable in a lower dimension can be linearly separable in a higher dimension - which is why we conduct the transform. Let us experiment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A transformation that lifts the data into a higher-dimensional space is called a *kernel*. A polynomial kernel expands the feature space by computing all the polynomial cross terms to a specific degree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2) Poly SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train acc:  0.996076190476\n",
      "test acc:  0.980514285714\n",
      "Wall time: 2min 12s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "psvc = SVC(kernel='poly', degree=3, tol=0.01, cache_size=4000)\n",
    "## YOUR CODE HERE - fit the psvc model\n",
    "psvc.fit(X_train_pca,y_train)\n",
    "## YOUR CODE HERE - print training accuracy\n",
    "## YOUR CODE HERE - print test accuracy\n",
    "print('train acc: ', accuracy_score(psvc.predict(X_train_pca), y_train))\n",
    "print('test acc: ', accuracy_score(psvc.predict(X_test_pca), y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Play around with the degree of the polynomial kernel to see what accuracy you can get."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3) RBF SVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The RBF kernel uses the gaussian function to create an infinite dimensional space - a gaussian peak at each datapoint. Now fiddle with the `C` and `gamma` parameters of the gaussian kernel below to see what you can get. [Here's documentation](http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train acc:  0.9928\n",
      "test acc:  0.982685714286\n",
      "Wall time: 2min 11s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "rsvc = SVC(kernel='rbf', tol=0.01, cache_size=4000)\n",
    "rsvc.fit(X_train_pca,y_train)\n",
    "## YOUR CODE HERE - fit the rsvc model\n",
    "## YOUR CODE HERE - print training accuracy\n",
    "## YOUR CODE HERE - print test accuracy\n",
    "print('train acc: ', accuracy_score(rsvc.predict(X_train_pca), y_train))\n",
    "print('test acc: ', accuracy_score(rsvc.predict(X_test_pca), y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Isn't that just amazing accuracy?\n",
    "Yes it is!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should never do neural networks in sklearn. Use Pytorch, Keras, etc. However, we will use sklearn for demonstrative purposes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic neural networks proceed in layers. Each layer has a certain number of nodes, representing how expressive that layer can be. Below is a sample network, with an input layer, one hidden (middle) layer of 50 neurons, and finally the output layer.\n",
    "\n",
    "# Isn't a hidden layer 100 neurons?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4) Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.57643377\n",
      "Iteration 2, loss = 0.18981556\n",
      "Iteration 3, loss = 0.13796627\n",
      "Iteration 4, loss = 0.10882780\n",
      "Iteration 5, loss = 0.09070660\n",
      "Iteration 6, loss = 0.07663921\n",
      "Iteration 7, loss = 0.06623248\n",
      "Iteration 8, loss = 0.05819505\n",
      "Iteration 9, loss = 0.05131167\n",
      "Iteration 10, loss = 0.04583143\n",
      "Iteration 11, loss = 0.03997059\n",
      "Iteration 12, loss = 0.03605682\n",
      "Iteration 13, loss = 0.03157209\n",
      "Iteration 14, loss = 0.02783151\n",
      "Iteration 15, loss = 0.02469605\n",
      "Iteration 16, loss = 0.02185288\n",
      "Iteration 17, loss = 0.01976651\n",
      "Iteration 18, loss = 0.01705946\n",
      "Iteration 19, loss = 0.01517791\n",
      "Iteration 20, loss = 0.01326950\n",
      "Iteration 21, loss = 0.01152645\n",
      "Iteration 22, loss = 0.01031080\n",
      "Iteration 23, loss = 0.00916490\n",
      "Iteration 24, loss = 0.00831506\n",
      "Iteration 25, loss = 0.00817845\n",
      "Iteration 26, loss = 0.00692194\n",
      "Iteration 27, loss = 0.00677176\n",
      "Iteration 28, loss = 0.00554677\n",
      "Iteration 29, loss = 0.00415878\n",
      "Iteration 30, loss = 0.00442816\n",
      "Iteration 31, loss = 0.00430385\n",
      "Iteration 32, loss = 0.00547239\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "train acc:  0.999352380952\n",
      "test acc:  0.973714285714\n"
     ]
    }
   ],
   "source": [
    "nn = MLPClassifier(hidden_layer_sizes=(90,199), solver='adam', verbose=1)\n",
    "## YOUR CODE HERE - fit the nn\n",
    "nn.fit(X_train_pca,y_train)\n",
    "## YOUR CODE HERE - print training accuracy\n",
    "## YOUR CODE HERE - print test accuracy\n",
    "print('train acc: ', accuracy_score(nn.predict(X_train_pca), y_train))\n",
    "print('test acc: ', accuracy_score(nn.predict(X_test_pca), y_test))\n",
    "#hidden_layer_sizes=(90,200), 0.977485714286\n",
    "#hidden_layer_sizes=(80,200), solver='adam', verbose=1),test acc:  0.9736\n",
    "#nn = MLPClassifier(hidden_layer_sizes=(91,200), solver='adam', verbose=1), 0.975485714286\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fiddle around with the hiddle layers. Change the number of neurons, add more layers, experiment. You should be able to hit 98% accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural networks are optimized with a technique called gradient descent (a neural net is just one big function - so we can take the gradient with respect to all its parameters, then just go opposite the gradient to try and find the minimum). This is why it requires many iterations to converge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Turning In"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Convert this notebook to a regular python file (file -> download as -> python)\n",
    "\n",
    "2. Submit both the notebook and python file via a pull request as specified in the README"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": false,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
