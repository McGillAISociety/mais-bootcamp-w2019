{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This assignment provides a brief introduction to the machine learning process that we will cover in depth in this course. We want to show a simple workflow of how you can use the sklearn library for machine learning. We will be using the MNIST handwritten digits dataset to dive into the machine learning process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let us import some libraries.\n",
    "\n",
    "1) numpy - http://www.numpy.org/ - A library for dealing with N-dimensional arrays in python. Numpy provides efficient implementations of common numerical computations used in linear algebra.\n",
    "\n",
    "2) sklearn - https://scikit-learn.org/stable/ - A data analysis library that provides implementations of many machine learning algorithms (and much more!).\n",
    "\n",
    "3) matplotlib - https://matplotlib.org/ - A python 2D plotting library used for visualizations (data, charts, etc.)\n",
    "\n",
    "These libraries (and many more) are often used together and built on top of each other. For example, sklearn depends on numpy and uses numpy arrays under the hood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from scipy import io\n",
    "from sklearn.datasets import fetch_mldata\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LinearRegression, Ridge, LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this homework assignment, we will be using the MNIST dataset. The MNIST data is a collection of black and white 28x28 images, each picturing a handwritten digit. These were collected from digits people write at the post office, and now this dataset is a standard benchmark to evaluate models against used in the machine learning community. We have provided the .mat file in the assignment repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(784, 70000)\n",
      "(1, 70000)\n"
     ]
    }
   ],
   "source": [
    "mnist = io.loadmat('mnist-original.mat', struct_as_record=True)\n",
    "X = mnist['data'].astype('float64') # Transpose the matrix because we want each row to be an example\n",
    "y = mnist['label'].astype('int64')\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = io.loadmat('mnist-original.mat', struct_as_record=True)\n",
    "X = mnist['data'].astype('float64').T # Transpose the matrix because we want each row to be an example\n",
    "y = mnist['label'].astype('int64').T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us first explore this data a little bit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(70000, 784) (70000, 1)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape, y.shape) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The X matrix here contains all the digit pictures. The data is (n_samples x n_features), meaning this data contains 70000 pictures, each with 784 features (the 28x28 image is flattened into a single row). The y vector contains the label for each digit, so we know which digit (or class - class means category) is in each picture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try and visualize this data a bit. Change around the index variable to explore more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.  51. 159. 253. 159.  50.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   48. 238. 252. 252. 252. 237.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  54.\n",
      "  227. 253. 252. 239. 233. 252.  57.   6.   0.   0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  10.  60. 224.\n",
      "  252. 253. 252. 202.  84. 252. 253. 122.   0.   0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0. 163. 252. 252.\n",
      "  252. 253. 252. 252.  96. 189. 253. 167.   0.   0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  51. 238. 253. 253.\n",
      "  190. 114. 253. 228.  47.  79. 255. 168.   0.   0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.   0.   0.   0.   0.  48. 238. 252. 252. 179.\n",
      "   12.  75. 121.  21.   0.   0. 253. 243.  50.   0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.   0.   0.   0.  38. 165. 253. 233. 208.  84.\n",
      "    0.   0.   0.   0.   0.   0. 253. 252. 165.   0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.   0.   0.   7. 178. 252. 240.  71.  19.  28.\n",
      "    0.   0.   0.   0.   0.   0. 253. 252. 195.   0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.   0.   0.  57. 252. 252.  63.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0. 253. 252. 195.   0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.   0.   0. 198. 253. 190.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0. 255. 253. 196.   0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.   0.  76. 246. 252. 112.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0. 253. 252. 148.   0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.   0.  85. 252. 230.  25.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   7. 135. 253. 186.  12.   0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.   0.  85. 252. 223.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   7. 131. 252. 225.  71.   0.   0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.   0.  85. 252. 145.   0.   0.   0.   0.   0.\n",
      "    0.   0.  48. 165. 252. 173.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.   0.  86. 253. 225.   0.   0.   0.   0.   0.\n",
      "    0. 114. 238. 253. 162.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.   0.  85. 252. 249. 146.  48.  29.  85. 178.\n",
      "  225. 253. 223. 167.  56.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.   0.  85. 252. 252. 252. 229. 215. 252. 252.\n",
      "  252. 196. 130.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.   0.  28. 199. 252. 252. 253. 252. 252. 233.\n",
      "  145.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.   0.   0.  25. 128. 252. 253. 252. 141.  37.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x132a8a240>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAEICAYAAACQ6CLfAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAEQFJREFUeJzt3X2wVPV9x/H3xyfiEwS1RUYkphY7Y1q9ClKmw0RimtSiHcg4PjBG6KQd7EyYGJs6VYNCWq0ZR23EjI5EqSgWiKIFrcZYMZpObSqiQdQmUgcVvIL4BMTUB/j2jz10LuTub/fu09nL7/Oa2bl7z3fPni8LH87Z8zu7P0UEZpaffcpuwMzK4fCbZcrhN8uUw2+WKYffLFMOv1mmHH6zTDn8eyFJP5H0l61eV9Llkm5r4DlD0q8kXV3n479TPD4k7TfQ7Vl9HP4uJmm9pD8uu49dIuIfIqKh/1SAEyPi27t+kdQj6RlJHxQ/e/psZw7wuWb7tTSH3zpO0gHAcmARMBxYCCwvlluHOPyDkKThkh6U9Jakd4v7o/Z42LGS/kvSVknLJR3WZ/0Jkv5D0nuSfi5pUp3bnStpUXH/U5IWSXq7eJ6nJY2o848wCdgP+F5EfBgR8wABp9W5vrWAwz847QP8E/AZYDTwa+D7ezxmOvA1YCTwCTAPQNJRwL8CVwGHAX8DLJP0WwPsYQYwDDgaOBz4q6KPenwOWBO7f7BkDT7U7yiHfxCKiLcjYllEfBAR24CrgVP3eNhdEbE2In4FXAGcI2lf4KvAQxHxUETsjIhHgVXA5AG28TGV0P9uROyIiGciYmud6x4CvL/HsveBQwfYgzXB4R+EJB0k6VZJr0raCjwJfLoI9y6v97n/KrA/cASVo4Wzi0P19yS9B0ykcoQwEHcBjwBLJL0h6VpJ+9e57nZg6B7LhgLbBtiDNcHhH5y+Bfwe8IcRMRT4fLFcfR5zdJ/7o6nsqbdQ+U/hroj4dJ/bwRHx3YE0EBEfR8R3IuJ44I+AM6m81ajHC8AJkvr2e0Kx3DrE4e9++xcn13bd9qNyePxr4L3iRN6cftb7qqTjJR0E/B1wb0TsoHKG/c8k/YmkfYvnnNTPCcMkSV+Q9AfF0cZWKv+57Kxz9Z8AO4BvSBoiaVaxfOVAerDmOPzd7yEqQd91mwt8DziQyp78P4Ef9bPeXcAdwJvAp4BvAETE68AU4HLgLSpHApcw8H8LRwL3Ugn+S8ATxTZrioiPgKlUjhTeo3Jicmqx3DpE/iYfazdJ/wt8CMyLiCvqePwc4K+BIcDBxRGLtZjDb5YpH/abZcrhN8tURz8xJcnvMczaLCJU+1FN7vklnS7pF5LWSbq0mecys85q+IRfMb77S+BLwAbgaWBaRLyYWMd7frM268SefzywLiJeKcZnl1AZPzazQaCZ8B/F7tePbyiW7UbSTEmrJK1qYltm1mJtP+EXEfOB+eDDfrNu0syefyO7f3hkVLHMzAaBZsL/NDBG0meLr186D1jRmrbMrN0aPuyPiE+KT2M9AuwLLIgIfyTTbJDo6LX9fs9v1n4ducjHzAYvh98sUw6/WaYcfrNMOfxmmXL4zTLl8JtlyuE3y5TDb5Yph98sUw6/WaYcfrNMOfxmmXL4zTLl8JtlyuE3y5TDb5Yph98sUw6/WaYcfrNMOfxmmeroFN229xk7dmyyPmvWrKq16dOnJ9e98847k/WbbropWV+9enWynjvv+c0y5fCbZcrhN8uUw2+WKYffLFMOv1mmHH6zTHmWXkvq6elJ1leuXJmsDx06tJXt7Ob9999P1g8//PC2bbub1TtLb1MX+UhaD2wDdgCfRMS4Zp7PzDqnFVf4fSEitrTgecysg/ye3yxTzYY/gB9LekbSzP4eIGmmpFWSVjW5LTNroWYP+ydGxEZJvw08Kum/I+LJvg+IiPnAfPAJP7Nu0tSePyI2Fj83A/cD41vRlJm1X8Phl3SwpEN33Qe+DKxtVWNm1l7NHPaPAO6XtOt5/jkiftSSrqxjxo9PH6wtW7YsWR82bFiynrqOZNu2bcl1P/roo2S91jj+hAkTqtZqfda/1rb3Bg2HPyJeAU5sYS9m1kEe6jPLlMNvlimH3yxTDr9Zphx+s0z5I717gYMOOqhq7eSTT06uu2jRomR91KhRyXox1FtV6t9XreG2a6+9NllfsmRJsp7qbfbs2cl1r7nmmmS9m9X7kV7v+c0y5fCbZcrhN8uUw2+WKYffLFMOv1mmHH6zTHmK7r3ArbfeWrU2bdq0DnYyMLWuQTjkkEOS9SeeeCJZnzRpUtXaCSeckFw3B97zm2XK4TfLlMNvlimH3yxTDr9Zphx+s0w5/GaZ8jj/IDB27Nhk/Ywzzqhaq/V5+1pqjaU/8MADyfp1111XtfbGG28k13322WeT9XfffTdZP+2006rWmn1d9gbe85tlyuE3y5TDb5Yph98sUw6/WaYcfrNMOfxmmfL39neBnp6eZH3lypXJ+tChQxve9sMPP5ys1/o+gFNPPTVZT31u/rbbbkuu+9ZbbyXrtezYsaNq7YMPPkiuW+vPVWvOgTK17Hv7JS2QtFnS2j7LDpP0qKSXi5/Dm2nWzDqvnsP+O4DT91h2KfBYRIwBHit+N7NBpGb4I+JJ4J09Fk8BFhb3FwJTW9yXmbVZo9f2j4iI3uL+m8CIag+UNBOY2eB2zKxNmv5gT0RE6kReRMwH5oNP+Jl1k0aH+jZJGglQ/NzcupbMrBMaDf8KYEZxfwawvDXtmFmn1Bznl7QYmAQcAWwC5gD/AvwQGA28CpwTEXueFOzvubI87D/uuOOS9Tlz5iTr5513XrK+ZcuWqrXe3t6qNYCrrroqWb/33nuT9W6WGuev9e9+6dKlyfr555/fUE+dUO84f833/BFR7SqPLw6oIzPrKr681yxTDr9Zphx+s0w5/GaZcvjNMuWv7m6BIUOGJOupr68GmDx5crK+bdu2ZH369OlVa6tWrUque+CBBybruRo9enTZLbSd9/xmmXL4zTLl8JtlyuE3y5TDb5Yph98sUw6/WaY8zt8CJ510UrJeaxy/lilTpiTrtabRNuuP9/xmmXL4zTLl8JtlyuE3y5TDb5Yph98sUw6/WaY8zt8CN9xwQ7Iupb9JudY4vcfxG7PPPtX3bTt37uxgJ93Je36zTDn8Zply+M0y5fCbZcrhN8uUw2+WKYffLFMe56/TmWeeWbXW09OTXLfWdNArVqxoqCdLS43l1/o7ee6551rdTtepueeXtEDSZklr+yybK2mjpOeKW3PfVmFmHVfPYf8dwOn9LP/HiOgpbg+1ti0za7ea4Y+IJ4F3OtCLmXVQMyf8ZklaU7wtGF7tQZJmSlolKT1pnJl1VKPhvwU4FugBeoHrqz0wIuZHxLiIGNfgtsysDRoKf0RsiogdEbET+AEwvrVtmVm7NRR+SSP7/PoVYG21x5pZd6o5zi9pMTAJOELSBmAOMElSDxDAeuDCNvbYFVLz2B9wwAHJdTdv3pysL126tKGe9nZDhgxJ1ufOndvwc69cuTJZv+yyyxp+7sGiZvgjYlo/i29vQy9m1kG+vNcsUw6/WaYcfrNMOfxmmXL4zTLlj/R2wIcffpis9/b2dqiT7lJrKG/27NnJ+iWXXJKsb9iwoWrt+uurXpQKwPbt25P1vYH3/GaZcvjNMuXwm2XK4TfLlMNvlimH3yxTDr9ZpjzO3wE5fzV36mvNa43Tn3vuucn68uXLk/WzzjorWc+d9/xmmXL4zTLl8JtlyuE3y5TDb5Yph98sUw6/WaY8zl8nSQ3VAKZOnZqsX3TRRQ311A0uvvjiZP2KK66oWhs2bFhy3bvvvjtZnz59erJuad7zm2XK4TfLlMNvlimH3yxTDr9Zphx+s0w5/GaZqmeK7qOBO4ERVKbknh8RN0o6DFgKHENlmu5zIuLd9rVarohoqAZw5JFHJuvz5s1L1hcsWJCsv/3221VrEyZMSK57wQUXJOsnnnhisj5q1Khk/bXXXqtae+SRR5Lr3nzzzcm6NaeePf8nwLci4nhgAvB1SccDlwKPRcQY4LHidzMbJGqGPyJ6I2J1cX8b8BJwFDAFWFg8bCGQvozNzLrKgN7zSzoGOAn4GTAiInbNM/UmlbcFZjZI1H1tv6RDgGXANyNia9/r2SMiJPX7xlfSTGBms42aWWvVteeXtD+V4N8dEfcVizdJGlnURwKb+1s3IuZHxLiIGNeKhs2sNWqGX5Vd/O3ASxFxQ5/SCmBGcX8GkP4qVTPrKqo1TCVpIvBT4HlgZ7H4cirv+38IjAZepTLU906N50pvrIudffbZVWuLFy9u67Y3bdqUrG/durVqbcyYMa1uZzdPPfVUsv74449XrV155ZWtbseAiEh/xrxQ8z1/RPw7UO3JvjiQpsyse/gKP7NMOfxmmXL4zTLl8JtlyuE3y5TDb5apmuP8Ld3YIB7nT3109Z577kmue8oppzS17VpfDd7M32Hq48AAS5YsSdYH89eO763qHef3nt8sUw6/WaYcfrNMOfxmmXL4zTLl8JtlyuE3y5TH+Vtg5MiRyfqFF16YrM+ePTtZb2ac/8Ybb0yue8sttyTr69atS9at+3ic38ySHH6zTDn8Zply+M0y5fCbZcrhN8uUw2+WKY/zm+1lPM5vZkkOv1mmHH6zTDn8Zply+M0y5fCbZcrhN8tUzfBLOlrS45JelPSCpIuK5XMlbZT0XHGb3P52zaxVal7kI2kkMDIiVks6FHgGmAqcA2yPiOvq3pgv8jFru3ov8tmvjifqBXqL+9skvQQc1Vx7Zla2Ab3nl3QMcBLws2LRLElrJC2QNLzKOjMlrZK0qqlOzayl6r62X9IhwBPA1RFxn6QRwBYggL+n8tbgazWew4f9Zm1W72F/XeGXtD/wIPBIRNzQT/0Y4MGI+P0az+Pwm7VZyz7Yo8pXx94OvNQ3+MWJwF2+AqwdaJNmVp56zvZPBH4KPA/sLBZfDkwDeqgc9q8HLixODqaey3t+szZr6WF/qzj8Zu3nz/ObWZLDb5Yph98sUw6/WaYcfrNMOfxmmXL4zTLl8JtlyuE3y5TDb5Yph98sUw6/WaYcfrNMOfxmmar5BZ4ttgV4tc/vRxTLulG39tatfYF7a1Qre/tMvQ/s6Of5f2Pj0qqIGFdaAwnd2lu39gXurVFl9ebDfrNMOfxmmSo7/PNL3n5Kt/bWrX2Be2tUKb2V+p7fzMpT9p7fzEri8JtlqpTwSzpd0i8krZN0aRk9VCNpvaTni2nHS51fsJgDcbOktX2WHSbpUUkvFz/7nSOxpN66Ytr2xLTypb523Tbdfcff80vaF/gl8CVgA/A0MC0iXuxoI1VIWg+Mi4jSLwiR9HlgO3DnrqnQJF0LvBMR3y3+4xweEX/bJb3NZYDTtrept2rTyv85Jb52rZzuvhXK2POPB9ZFxCsR8RGwBJhSQh9dLyKeBN7ZY/EUYGFxfyGVfzwdV6W3rhARvRGxuri/Ddg1rXypr12ir1KUEf6jgNf7/L6BEl+AfgTwY0nPSJpZdjP9GNFnWrQ3gRFlNtOPmtO2d9Ie08p3zWvXyHT3reYTfr9pYkScDPwp8PXi8LYrReU9WzeN1d4CHEtlDsde4PoymymmlV8GfDMitvatlfna9dNXKa9bGeHfCBzd5/dRxbKuEBEbi5+bgfupvE3pJpt2zZBc/Nxccj//LyI2RcSOiNgJ/IASX7tiWvllwN0RcV+xuPTXrr++ynrdygj/08AYSZ+VdABwHrCihD5+g6SDixMxSDoY+DLdN/X4CmBGcX8GsLzEXnbTLdO2V5tWnpJfu66b7j4iOn4DJlM54/8/wLfL6KFKX78D/Ly4vVB2b8BiKoeBH1M5N/IXwOHAY8DLwL8Bh3VRb3dRmcp9DZWgjSypt4lUDunXAM8Vt8llv3aJvkp53Xx5r1mmfMLPLFMOv1mmHH6zTDn8Zply+M0y5fCbZcrhN8vU/wFHJn0Qe3fwmAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "index = 0 #15000, 28999, 67345\n",
    "image = X[index].reshape((28, 28))\n",
    "print(image) # Shows the pixel values at each cell in the matrix. Ranges from 0-255\n",
    "plt.title('Label is ' + str(y[index]))\n",
    "plt.imshow(image, cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that each pixel value ranges from 0-255. When we train our models, a good practice is to *standardize* the data so different features can be compared more equally (it also speeds up computation). Here we will use a simple standardization, squeezing all values into the [0, 1] interval range. This kind of standardization is called min-max normalization. For other methods, see https://en.wikipedia.org/wiki/Feature_scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X / 255 # Shorthand for dividing all values in the X matrix by 255. Numpy provides lots of shortcuts like this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we train our model, we want it to have the lowest error. Error presents itself in 2 ways: bias (how close our model is to the ideal model), and variance (how much our model varies with different datasets). If we train our model on a chunk of data, and then test our model on that same data, we will only witness the first type of error - bias. However, if we test on new, unseen data, that will reflect both bias and variance. This is the reasoning behind cross validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we want to have 2 datasets, train and test, each used for the named purpose exclusively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(52500, 784) (17500, 784)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will walk you through applying various models to try and achieve the lowest error rate on this data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each of our labels is a number from 0-9. That is, our labels are categorical. If we simply did regression on this data, the labels would imply some sort of ordering and distance between the classes (imagine we were classing colors instead, there is no notion of distance or order i.e. 8 is not \"more\" of a digit than 7). We can fix this issue by one-hot encoding our labels. So, instead of each label being a simple digit, each label is a vector of 10 entries. 9 of those entries are zero, and only 1 entry is equal to one, corresponding to the index of the digit. This way, the distance between the labels are the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ethanitovitch/.mais-env/lib/python3.7/site-packages/sklearn/preprocessing/_encoders.py:368: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(70000, 10)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc = OneHotEncoder(sparse=False)\n",
    "y_hot = enc.fit_transform(y.reshape(-1, 1))\n",
    "y_train_hot = enc.transform(y_train.reshape(-1, 1))\n",
    "y_hot.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember how the first sample is the digit zero? Let's now look at the new label at that index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hot[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 3 steps to build your model: \n",
    "\n",
    "1) Create the model\n",
    "\n",
    "2) Train the model\n",
    "\n",
    "3) Use your model to make predictions\n",
    "\n",
    "In the sklearn API, this is made very clear. First you instantiate the model (constructor), then you call train it with the `fit` method, then you can make predictions on new data with the `test` method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's do a basic linear regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instantiate the model\n",
    "linear = LinearRegression()\n",
    "# Fit the model (train)\n",
    "linear.fit(X_train, y_train_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('train acc: ', 0.8592190476190477)\n",
      "('test acc: ', 0.848)\n"
     ]
    }
   ],
   "source": [
    "# use trained model to predict both train and test sets\n",
    "y_train_pred = linear.predict(X_train)\n",
    "y_test_pred = linear.predict(X_test)\n",
    "\n",
    "# print accuracies\n",
    "print('train acc: ', accuracy_score(y_train_pred.argmax(axis=1), y_train))\n",
    "print('test acc: ', accuracy_score(y_test_pred.argmax(axis=1), y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note on interpretability: you can view the weights of your model with `linear.coef_`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ridge Regression (L2) is one method of preventing a common problem in machine learning called \"overfitting\". Remember when we split our examples into training and test sets? Overfitting occurs when the model performs well on the training set but not on the test set. This means that the model does not generalize well to previously unseen examples.\n",
    "\n",
    "Let us try Ridge Regression and see if we get better results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('train acc: ', 0.8590666666666666)\n",
      "('test acc: ', 0.8485714285714285)\n"
     ]
    }
   ],
   "source": [
    "ridge = Ridge(alpha=0.1)\n",
    "ridge.fit(X_train, y_train_hot)\n",
    "print('train acc: ', accuracy_score(ridge.predict(X_train).argmax(axis=1), y_train))\n",
    "print('test acc: ', accuracy_score(ridge.predict(X_test).argmax(axis=1), y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The alpha controls how much to penalize the weights. Play around with it to see if you can improve the test accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you have seen how to use some basic models to fit and evaluate your data. You will now walk through working with more models. Fill in code where needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now do logistic regression. From now on, the models will automatically one-hot the labels (so we don't need to worry about it)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda2/lib/python2.7/site-packages/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('train acc: ', 0.9186857142857143)\n",
      "('test acc: ', 0.9164571428571429)\n"
     ]
    }
   ],
   "source": [
    "logreg = LogisticRegression(C=0.01, multi_class='multinomial', solver='saga', tol=0.1)\n",
    "logreg.fit(X_train, y_train)\n",
    "print('train acc: ', accuracy_score(logreg.predict(X_train), y_train))\n",
    "print('test acc: ', accuracy_score(logreg.predict(X_test), y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our accuracy has jumped ~5%! Why is this? Logistic Regression is a more complex model - instead of computing raw scores as in linear regression, it does one extra step and squashes values between 0 and 1. This means our model now optimizes over *probabilities* instead of raw scores. This makes sense since our vectors are 1-hot encoded."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The C hyperparameter controls inverse regularization strength (inverse for this model only). Reguralization is important to make sure our model doesn't overfit (perform much better on train data than test data). Play around with the C parameter to try and get better results! You should be able to hit 92%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1) Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision Trees are a completely different type of classifier. They essentially break up the possible space by repeatedly \"splitting\" on features to keep narrowing down the possibilities. Decision Trees are normally individually very week, so we typically average them together in bunches called Random Forest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you have seen many examples for how to construct, fit, and evaluate a model. Now do the same for Random Forest using the [documentation here](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html). You should be able to create one easily without needing to specify any constructor parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda2/lib/python2.7/site-packages/ipykernel_launcher.py:4: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('train acc: ', 0.9999619047619047)\n",
      "('test acc: ', 0.9669714285714286)\n"
     ]
    }
   ],
   "source": [
    "## YOUR CODE HERE - call the constructor\n",
    "rfc = RandomForestClassifier(n_estimators=100, min_samples_split=4)\n",
    "## YOUR CODE HERE - fit the rf model (just like logistic regression)\n",
    "rfc.fit(X_train, y_train)\n",
    "## YOUR CODE HERE - print training accuracy\n",
    "## YOUR CODE HERE - print test accuracy\n",
    "print('train acc: ', accuracy_score(rfc.predict(X_train), y_train))\n",
    "print('test acc: ', accuracy_score(rfc.predict(X_test), y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That train accuracy is amazing, let's see if we can boost up the test accuracy a bit (since that's what really counts). Try and play around with the hyperparameters to see if you can edge out more accuracy (look at the documentation for parameters in the constructor). Focus on `n_estimators`, `min_samples_split`, `max_features`. You should be able to hit ~97%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A support vector classifier is another completely different type of classifier. It tries to find the best separating hyperplane through your data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The SVC will toast our laptops unless we reduce the data dimensionality. Let's keep 80% of the variation, and get rid of the rest. (This will cause a slight drop in performance, but not by much)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=0.8, whiten=True)\n",
    "X_train_pca = pca.fit_transform(X_train)\n",
    "X_test_pca = pca.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! Now let's take a look at what that actually did."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(52500, 43)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_pca.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember, before we had 784 (28x28) features! However, PCA found just 43 basis features that explain 80% of the data. So, we went to just 5% of the original input space, but we still retained 80% of the information! Nice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This [blog post](http://colah.github.io/posts/2014-10-Visualizing-MNIST/) explains dimensionality reduction with MNIST quite well. It's a short read (<10 mins), and it contains some pretty cool visualizations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's train our first SVC. The LinearSVC can only find a linear decision boundary (the hyperplane)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('train acc: ', 0.8926095238095239)\n",
      "('test acc: ', 0.8922285714285715)\n"
     ]
    }
   ],
   "source": [
    "lsvc = LinearSVC(dual=False, tol=0.01)\n",
    "lsvc.fit(X_train_pca, y_train)\n",
    "print('train acc: ', accuracy_score(lsvc.predict(X_train_pca), y_train))\n",
    "print('test acc: ', accuracy_score(lsvc.predict(X_test_pca), y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVMs are really interesting because they have something called the *dual formulation*, in which the computation is expressed as training point inner products. This means that data can be lifted into higher dimensions easily with this \"kernel trick\". Data that is not linearly separable in a lower dimension can be linearly separable in a higher dimension - which is why we conduct the transform. Let us experiment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A transformation that lifts the data into a higher-dimensional space is called a *kernel*. A polynomial kernel expands the feature space by computing all the polynomial cross terms to a specific degree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2) Poly SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('train acc: ', 0.9962666666666666)\n",
      "('test acc: ', 0.9813714285714286)\n"
     ]
    }
   ],
   "source": [
    "psvc = SVC(kernel='poly', degree=3, tol=0.01, cache_size=4000)\n",
    "## YOUR CODE HERE - fit the psvc model\n",
    "psvc.fit(X_train_pca, y_train)\n",
    "## YOUR CODE HERE - print training accuracy\n",
    "## YOUR CODE HERE - print test accuracy\n",
    "print('train acc: ', accuracy_score(psvc.predict(X_train_pca), y_train))\n",
    "print('test acc: ', accuracy_score(psvc.predict(X_test_pca), y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Play around with the degree of the polynomial kernel to see what accuracy you can get."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3) RBF SVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The RBF kernel uses the gaussian function to create an infinite dimensional space - a gaussian peak at each datapoint. Now fiddle with the `C` and `gamma` parameters of the gaussian kernel below to see what you can get. [Here's documentation](http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('train acc: ', 0.9929333333333333)\n",
      "('test acc: ', 0.9822857142857143)\n"
     ]
    }
   ],
   "source": [
    "rsvc = SVC(kernel='rbf', tol=0.01, cache_size=4000)\n",
    "## YOUR CODE HERE - fit the rsvc model\n",
    "rsvc.fit(X_train_pca, y_train)\n",
    "## YOUR CODE HERE - print training accuracy\n",
    "## YOUR CODE HERE - print test accuracy\n",
    "print('train acc: ', accuracy_score(rsvc.predict(X_train_pca), y_train))\n",
    "print('test acc: ', accuracy_score(rsvc.predict(X_test_pca), y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Isn't that just amazing accuracy?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should never do neural networks in sklearn. Use Pytorch, Keras, etc. However, we will use sklearn for demonstrative purposes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic neural networks proceed in layers. Each layer has a certain number of nodes, representing how expressive that layer can be. Below is a sample network, with an input layer, one hidden (middle) layer of 50 neurons, and finally the output layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4) Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda2/lib/python2.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:912: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.21247945\n",
      "Iteration 2, loss = 0.40385418\n",
      "Iteration 3, loss = 0.29197202\n",
      "Iteration 4, loss = 0.24464269\n",
      "Iteration 5, loss = 0.21479175\n",
      "Iteration 6, loss = 0.19309225\n",
      "Iteration 7, loss = 0.17596990\n",
      "Iteration 8, loss = 0.16213775\n",
      "Iteration 9, loss = 0.15062599\n",
      "Iteration 10, loss = 0.14063231\n",
      "Iteration 11, loss = 0.13243484\n",
      "Iteration 12, loss = 0.12507485\n",
      "Iteration 13, loss = 0.11909483\n",
      "Iteration 14, loss = 0.11360040\n",
      "Iteration 15, loss = 0.10886043\n",
      "Iteration 16, loss = 0.10443119\n",
      "Iteration 17, loss = 0.10055568\n",
      "Iteration 18, loss = 0.09708423\n",
      "Iteration 19, loss = 0.09412021\n",
      "Iteration 20, loss = 0.09123157\n",
      "Iteration 21, loss = 0.08850408\n",
      "Iteration 22, loss = 0.08627999\n",
      "Iteration 23, loss = 0.08402469\n",
      "Iteration 24, loss = 0.08193262\n",
      "Iteration 25, loss = 0.07987988\n",
      "Iteration 26, loss = 0.07821819\n",
      "Iteration 27, loss = 0.07642728\n",
      "Iteration 28, loss = 0.07498936\n",
      "Iteration 29, loss = 0.07356369\n",
      "Iteration 30, loss = 0.07208828\n",
      "Iteration 31, loss = 0.07045373\n",
      "Iteration 32, loss = 0.06918135\n",
      "Iteration 33, loss = 0.06805207\n",
      "Iteration 34, loss = 0.06693809\n",
      "Iteration 35, loss = 0.06578600\n",
      "Iteration 36, loss = 0.06476030\n",
      "Iteration 37, loss = 0.06365058\n",
      "Iteration 38, loss = 0.06261036\n",
      "Iteration 39, loss = 0.06175161\n",
      "Iteration 40, loss = 0.06082983\n",
      "Iteration 41, loss = 0.05996419\n",
      "Iteration 42, loss = 0.05905929\n",
      "Iteration 43, loss = 0.05845983\n",
      "Iteration 44, loss = 0.05760878\n",
      "Iteration 45, loss = 0.05672742\n",
      "Iteration 46, loss = 0.05618681\n",
      "Iteration 47, loss = 0.05548892\n",
      "Iteration 48, loss = 0.05482146\n",
      "Iteration 49, loss = 0.05413839\n",
      "Iteration 50, loss = 0.05357970\n",
      "Iteration 51, loss = 0.05282085\n",
      "Iteration 52, loss = 0.05219964\n",
      "Iteration 53, loss = 0.05180156\n",
      "Iteration 54, loss = 0.05127882\n",
      "Iteration 55, loss = 0.05050924\n",
      "Iteration 56, loss = 0.05009809\n",
      "Iteration 57, loss = 0.04984221\n",
      "Iteration 58, loss = 0.04917877\n",
      "Iteration 59, loss = 0.04853824\n",
      "Iteration 60, loss = 0.04823510\n",
      "Iteration 61, loss = 0.04765508\n",
      "Iteration 62, loss = 0.04729143\n",
      "Iteration 63, loss = 0.04697402\n",
      "Iteration 64, loss = 0.04637529\n",
      "Iteration 65, loss = 0.04610781\n",
      "Iteration 66, loss = 0.04557795\n",
      "Iteration 67, loss = 0.04505698\n",
      "Iteration 68, loss = 0.04507120\n",
      "Iteration 69, loss = 0.04450247\n",
      "Iteration 70, loss = 0.04407025\n",
      "Iteration 71, loss = 0.04382893\n",
      "Iteration 72, loss = 0.04347709\n",
      "Iteration 73, loss = 0.04312590\n",
      "Iteration 74, loss = 0.04287746\n",
      "Iteration 75, loss = 0.04232915\n",
      "Iteration 76, loss = 0.04211892\n",
      "Iteration 77, loss = 0.04155553\n",
      "Iteration 78, loss = 0.04138847\n",
      "Iteration 79, loss = 0.04126188\n",
      "Iteration 80, loss = 0.04080105\n",
      "Iteration 81, loss = 0.04046266\n",
      "Iteration 82, loss = 0.04024882\n",
      "Iteration 83, loss = 0.03993552\n",
      "Iteration 84, loss = 0.03979297\n",
      "Iteration 85, loss = 0.03926493\n",
      "Iteration 86, loss = 0.03910882\n",
      "Iteration 87, loss = 0.03881071\n",
      "Iteration 88, loss = 0.03879783\n",
      "Iteration 89, loss = 0.03844660\n",
      "Iteration 90, loss = 0.03826174\n",
      "Iteration 91, loss = 0.03795859\n",
      "Iteration 92, loss = 0.03757005\n",
      "Iteration 93, loss = 0.03742524\n",
      "Iteration 94, loss = 0.03717332\n",
      "Iteration 95, loss = 0.03714340\n",
      "Iteration 96, loss = 0.03688405\n",
      "Iteration 97, loss = 0.03644735\n",
      "Iteration 98, loss = 0.03638095\n",
      "Iteration 99, loss = 0.03616046\n",
      "Iteration 100, loss = 0.03607286\n",
      "Iteration 101, loss = 0.03557294\n",
      "Iteration 102, loss = 0.03567177\n",
      "Iteration 103, loss = 0.03536839\n",
      "Iteration 104, loss = 0.03519765\n",
      "Iteration 105, loss = 0.03490387\n",
      "Iteration 106, loss = 0.03472067\n",
      "Iteration 107, loss = 0.03439095\n",
      "Iteration 108, loss = 0.03446681\n",
      "Iteration 109, loss = 0.03420715\n",
      "Iteration 110, loss = 0.03410327\n",
      "Iteration 111, loss = 0.03390748\n",
      "Iteration 112, loss = 0.03358966\n",
      "Iteration 113, loss = 0.03361805\n",
      "Iteration 114, loss = 0.03330422\n",
      "Iteration 115, loss = 0.03322439\n",
      "Iteration 116, loss = 0.03305658\n",
      "Iteration 117, loss = 0.03269523\n",
      "Iteration 118, loss = 0.03266085\n",
      "Iteration 119, loss = 0.03280519\n",
      "Iteration 120, loss = 0.03251640\n",
      "Iteration 121, loss = 0.03201015\n",
      "Iteration 122, loss = 0.03232859\n",
      "Iteration 123, loss = 0.03200766\n",
      "Iteration 124, loss = 0.03180352\n",
      "Iteration 125, loss = 0.03171928\n",
      "Iteration 126, loss = 0.03164122\n",
      "Iteration 127, loss = 0.03136738\n",
      "Iteration 128, loss = 0.03136145\n",
      "Iteration 129, loss = 0.03131696\n",
      "Iteration 130, loss = 0.03120429\n",
      "Iteration 131, loss = 0.03093607\n",
      "Iteration 132, loss = 0.03083769\n",
      "Iteration 133, loss = 0.03067310\n",
      "Iteration 134, loss = 0.03056478\n",
      "Iteration 135, loss = 0.03049772\n",
      "Iteration 136, loss = 0.03028098\n",
      "Iteration 137, loss = 0.03012343\n",
      "Iteration 138, loss = 0.03008863\n",
      "Iteration 139, loss = 0.02975084\n",
      "Iteration 140, loss = 0.03000096\n",
      "Iteration 141, loss = 0.02992345\n",
      "Iteration 142, loss = 0.02971760\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "('train acc: ', 0.9934857142857143)\n",
      "('test acc: ', 0.9685714285714285)\n"
     ]
    }
   ],
   "source": [
    "nn = MLPClassifier(hidden_layer_sizes=(50,), solver='adam', verbose=1)\n",
    "## YOUR CODE HERE - fit the nn\n",
    "nn.fit(X_train_pca, y_train)\n",
    "## YOUR CODE HERE - print training accuracy\n",
    "## YOUR CODE HERE - print test accuracy\n",
    "print('train acc: ', accuracy_score(nn.predict(X_train_pca), y_train))\n",
    "print('test acc: ', accuracy_score(nn.predict(X_test_pca), y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fiddle around with the hiddle layers. Change the number of neurons, add more layers, experiment. You should be able to hit 98% accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural networks are optimized with a technique called gradient descent (a neural net is just one big function - so we can take the gradient with respect to all its parameters, then just go opposite the gradient to try and find the minimum). This is why it requires many iterations to converge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Turning In"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Convert this notebook to a regular python file (file -> download as -> python)\n",
    "\n",
    "2. Submit both the notebook and python file via a pull request as specified in the README"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (MAIS-202)",
   "language": "python",
   "name": ".mais-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": false,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
