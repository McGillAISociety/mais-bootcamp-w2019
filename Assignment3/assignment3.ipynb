{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 3: Introduction to PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch is a framework for creating and training neural networks. It's one of the most common neural network libraries, alongside TensorFlow, and is used extensively in both academia and industry. In this homework, we'll explore the basic operations within PyTorch, and we'll design a neural network to classify images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by importing the libraries that we'll need:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you can't import torch, go to www.pytorch.org and follow the instructions there for downloading PyTorch. You can select CUDA Version as None, as we won't be working with any GPUs on this homework."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch: Tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In PyTorch, data is stored as multidimensional arrays, called tensors. Tensors are very similar to numpy's ndarrays, and they support many of the same operations. We can define tensors by explicity setting the values, using a python list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A:\n",
      "tensor([[ 1,  2],\n",
      "        [ 4, -3]])\n",
      "\n",
      "\n",
      "B:\n",
      "tensor([[ 3,  1],\n",
      "        [-2,  3]])\n"
     ]
    }
   ],
   "source": [
    "A = torch.tensor([[1, 2], [4, -3]])\n",
    "B = torch.tensor([[3, 1], [-2, 3]])\n",
    "\n",
    "print(\"A:\")\n",
    "print(A)\n",
    "\n",
    "print('\\n')\n",
    "\n",
    "print(\"B:\")\n",
    "print(B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just like numpy, PyTorch supports operations like addition, multiplication, transposition, dot products, and concatenation of tensors. Look up and fill in the operations for the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sum of A and B:\n",
      "tensor([[4, 3],\n",
      "        [2, 0]])\n",
      "\n",
      "Elementwise product of A and B:\n",
      "tensor([[ 3,  2],\n",
      "        [-8, -9]])\n",
      "\n",
      "Matrix product of A and B:\n",
      "tensor([[-1,  7],\n",
      "        [18, -5]])\n",
      "tensor([[-1,  7],\n",
      "        [18, -5]])\n",
      "\n",
      "Transposition of A:\n",
      "tensor([[ 1,  4],\n",
      "        [ 2, -3]])\n",
      "\n",
      "\n",
      "Concatenation of A and B in the 0th dimension:\n",
      "tensor([[ 1,  2],\n",
      "        [ 4, -3],\n",
      "        [ 3,  1],\n",
      "        [-2,  3]])\n",
      "\n",
      "\n",
      "Concatenation of A and B in the 1st dimension:\n",
      "tensor([[ 1,  2,  3,  1],\n",
      "        [ 4, -3, -2,  3]])\n"
     ]
    }
   ],
   "source": [
    "print(\"Sum of A and B:\")\n",
    "### YOUR CODE HERE\n",
    "\n",
    "print(A+B)\n",
    "print()\n",
    "print(\"Elementwise product of A and B:\")\n",
    "### YOUR CODE HERE\n",
    "\n",
    "print(A * B)\n",
    "print()\n",
    "print(\"Matrix product of A and B:\")\n",
    "### YOUR CODE HERE\n",
    "\n",
    "print(A.mm(B))\n",
    "print(A@B)\n",
    "print()\n",
    "print(\"Transposition of A:\")\n",
    "### YOUR CODE HERE\n",
    "print(torch.t(A))\n",
    "print('\\n')\n",
    "\n",
    "print(\"Concatenation of A and B in the 0th dimension:\")\n",
    "### YOUR CODE HERE\n",
    "print(torch.cat((A,B), dim=0))\n",
    "print('\\n')\n",
    "\n",
    "print(\"Concatenation of A and B in the 1st dimension:\")\n",
    "### YOUR CODE HERE\n",
    "print(torch.cat((A,B),dim=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch also has tools for creating large tensors automatically, without explicity specifying the values. Find the corresponding tensor initialzers and fill in below. Your print statements should look like the following:\n",
    "\n",
    "3x4x5 Tensor of Zeros:\n",
    "```\n",
    "tensor([[[0., 0., 0., 0., 0.],\n",
    "         [0., 0., 0., 0., 0.],\n",
    "         [0., 0., 0., 0., 0.],\n",
    "         [0., 0., 0., 0., 0.]],\n",
    "\n",
    "        [[0., 0., 0., 0., 0.],\n",
    "         [0., 0., 0., 0., 0.],\n",
    "         [0., 0., 0., 0., 0.],\n",
    "         [0., 0., 0., 0., 0.]],\n",
    "\n",
    "        [[0., 0., 0., 0., 0.],\n",
    "         [0., 0., 0., 0., 0.],\n",
    "         [0., 0., 0., 0., 0.],\n",
    "         [0., 0., 0., 0., 0.]]])\n",
    "```\n",
    "\n",
    "\n",
    "5x5 Tensor with random elements sampled from a standard normal distrubtion: (these should be randomly generated values)\n",
    "```\n",
    "tensor([[ 0.2850,  0.5033, -1.8570, -1.6525,  0.3613],\n",
    "        [-0.7505,  0.4573, -0.2454,  0.1668,  0.7241],\n",
    "        [ 0.2976,  0.9827, -0.4879, -1.1144, -1.8235],\n",
    "        [-0.0264,  0.7341, -0.2235,  0.5306,  0.8385],\n",
    "        [ 0.2740,  0.3522, -0.5244, -0.1132,  0.5135]])\n",
    "```\n",
    "\n",
    "Tensor created from a range:\n",
    "```\n",
    "tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3x4x5 Tensor of Zeros:\n",
      "tensor([[[0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0.]]])\n",
      "\n",
      "5x5 Tensor with random elements sampled from a standard normal distrubtion:\n",
      "\n",
      "\n",
      "tensor([[[ 0.8472, -0.5487,  0.5441, -0.4143,  0.3530],\n",
      "         [-2.3684, -1.3495, -1.4362,  0.4495,  0.7653],\n",
      "         [-0.4588, -1.2766, -1.1046,  0.6433, -0.4041],\n",
      "         [-0.9070, -0.8848, -0.7742,  0.9715,  0.1811]],\n",
      "\n",
      "        [[ 0.7950, -0.6463,  0.6498, -1.6940, -1.7301],\n",
      "         [ 0.4800, -0.3126, -0.9792, -0.7225, -0.9661],\n",
      "         [-0.2768,  0.3816, -0.1800, -0.9100,  0.5326],\n",
      "         [-0.3087,  2.5587, -0.5301,  1.4236, -0.1230]],\n",
      "\n",
      "        [[-1.1625,  1.1921,  0.2033, -0.1160,  1.8042],\n",
      "         [ 0.8395,  0.5573, -1.3308, -0.4594, -1.8995],\n",
      "         [-1.9661, -0.3845,  0.5698,  1.4794,  0.7207],\n",
      "         [ 0.7933, -0.7215, -1.0278,  0.0277,  1.1890]]])\n",
      "Tensor created from a range:\n",
      "tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n"
     ]
    }
   ],
   "source": [
    "print(\"3x4x5 Tensor of Zeros:\")\n",
    "### YOUR CODE HERE\n",
    "v = torch.zeros(3,4,5)\n",
    "print(v)\n",
    "print()\n",
    "\n",
    "print(\"5x5 Tensor with random elements sampled from a standard normal distrubtion:\")\n",
    "### YOUR CODE HERE\n",
    "\n",
    "print('\\n')\n",
    "v = torch.randn(3,4,5)\n",
    "print(v)\n",
    "print(\"Tensor created from a range:\")\n",
    "### YOUR CODE HERE\n",
    "v = torch.arange(10)\n",
    "print(v)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, use PyTorch tensors to complete the following computation:\n",
    "\n",
    "Create a tensor of integers from the range 0 to 99, inclusive. Add 0.5 to each element in the tensor, and square each element of the result. Then, negate each element of the tensor, and apply the exponential to each element (i.e., change each element x into e^x). Now, sum all the elements of the tensor. Multiply this tensor by 2 and square each element and print your result.\n",
    "\n",
    "If you're right, you should get something very close to $$\\pi \\approx 3.14 .$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3.1409)\n"
     ]
    }
   ],
   "source": [
    "val = torch.arange(100).float()\n",
    "val+=0.5 #Adds 0.5 to each element\n",
    "#val = val*val #Multiply element wise\n",
    "val = torch.pow(val,2)\n",
    "val *= -1 #Negate (times -1)\n",
    "torch.exp_(val) #Inplace for exp^x\n",
    "val=torch.sum(val)*2\n",
    "val=torch.pow(val,2)\n",
    "\n",
    "### YOUR CODE HERE\n",
    "\n",
    "print(val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll try writing a computation that's prevalent throughout a lot of deep learning algorithms - calculating the softmax function:\n",
    "$$softmax(x_i) = \\frac{e^{x_i}}{\\sum_{j = 0}^{n - 1} e^{x_j}}$$\n",
    "Calculate the softmax function for the $val$ tensor below where $n$ is the number of elements in $val$, and $x_i$ is each element in $val$. DO NOT use the built-in softmax function. We should end up with a tensor that represents a probability distribution that sums to 1. (hint: you should calculate the sum of the exponents first)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0001, 0.0002, 0.0006, 0.0016, 0.0043, 0.0116, 0.0315, 0.0856, 0.2326,\n",
      "        0.6321])\n",
      "tensor(1.0000)\n"
     ]
    }
   ],
   "source": [
    "val1 = torch.arange(10).float()\n",
    "num = torch.exp(val1)\n",
    "denum = torch.exp(val1).sum()\n",
    "result1=num/denum\n",
    "\n",
    "### YOUR CODE HERE\n",
    "\n",
    "print(result1)\n",
    "print(torch.sum(result1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do this, you'll need to use the PyTorch documentation at https://pytorch.org/docs/stable/torch.html. Luckily, PyTorch has very well-written docs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch: Autograd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Autograd is PyTorch's automatic differentiation tool: It allows us to compute gradients by keeping track of all the operations that have happened to a tensor. In the context of neural networks, we'll interpret these gradient calculations as backpropagating a loss through a network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To understand how autograd works, we first need to understand the idea of a __computation graph__. A computation graph is a directed, acyclic graph (DAG) that contains a blueprint of a sequence of operations. For a neural network, these computations consist of matrix multiplications, bias additions, ReLUs, softmaxes, etc. Nodes in this graph consist of the operations themselves, while the edges represent tensors that flow forward along this graph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In PyTorch, the creation of this graph is __dynamic__. This means that tensors themselves keep track of their own computational history, and this history is build as the tensors flow through the network; this is unlike TensorFlow, where an external controller keeps track of the entire computation graph. This dynamic creation of the computation graph allows for lots of cool control-flows that are not possible (or at least very difficult) in TensorFlow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](https://raw.githubusercontent.com/pytorch/pytorch/master/docs/source/_static/img/dynamic_graph.gif)\n",
    "<center>_Dynamic computation graphs are cool!_</center>\n",
    "_ _"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at a simple computation to see what autograd is doing. First, let's create two tensors and add them together. To signal to PyTorch that we want to build a computation graph, we must set the flag requires_grad to be True when creating a tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor([1, 2], dtype=torch.float, requires_grad=True)\n",
    "b = torch.tensor([8, 3], dtype=torch.float, requires_grad=True)\n",
    "\n",
    "c = a + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, since a and b are both part of our computation graph, c will automatically be added:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.requires_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we add a tensor to our computation graph in this way, our tensor now has a grad_fn attribute. This attribute tells autograd how this tensor was generated, and what tensor(s) this particular node was created from."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the case of c, its grad_fn is of type AddBackward1, PyTorch's notation for a tensor that was created by adding two tensors together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<ThAddBackward at 0x1d9d6c54eb8>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.grad_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every grad_fn has an attribute called next_functions: This attribute lets the grad_fn pass on its gradient to the tensors that were used to compute it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((<AccumulateGrad at 0x1d9d6c54f60>, 0),\n",
       " (<AccumulateGrad at 0x1d9d6c549e8>, 0))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.grad_fn.next_functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we extract the tensor values corresponding to each of these functions, we can see a and b! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 2.], requires_grad=True)\n",
      "tensor([8., 3.], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(c.grad_fn.next_functions[0][0].variable)\n",
    "print(c.grad_fn.next_functions[1][0].variable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this way, autograd allows a tensor to record its entire computational history, implicitly creating a computational graph -- All dynamically and on-the-fly!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch: Modules and Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In PyTorch, collections of operations are encapsulated as __modules__. One way to visualize a module is to take a section of a computational graph and collapse it into a single node. Not only are modules useful for encapsulation, they have the ability to keep track of tensors that are contained inside of them: To do this, simply wrap a tensor with the class torch.nn.Parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To define a module, we must subclass the type torch.nn.Module. In addition, we must define a _forward_ method that tells PyTorch how to traverse through a module."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, let's define a logistic regression module. This module will contain two parameters: The weight vector and the bias. Calling the _forward_ method will output a probability between zero and one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.randn(10))\n",
    "        self.bias = nn.Parameter(torch.randn(1))\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, vector):\n",
    "        return self.sigmoid(torch.dot(vector, self.weight) + self.bias)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we have fixed the dimension of our weight to be 10, so our module will only accept 10-dimensional data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now create a random vector and pass it through the module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "module = LogisticRegression() #Creates instance of model\n",
    "vector = torch.randn(10) #Create a torch tensor from normal distribution of size 10\n",
    "output = module(vector) #Fit data to model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.9905], grad_fn=<SigmoidBackward>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, say that our loss function is mean-squared-error and our target value is 1. We can then write our loss as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = (output - 1) ** 2 #We assume target value is 1. Thus we want to minimize it. Must Square to evade negative values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0001], grad_fn=<PowBackward0>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To minimize this loss, we just call loss.backward(), and all the gradients will be computed for us! Note that wrapping a tensor as a Parameter will automatically set requires_grad = True."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward() #Calculate the backward gradients to minimize the loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.0001,  0.0003,  0.0001,  0.0002, -0.0002, -0.0003, -0.0000,  0.0003,\n",
      "        -0.0001, -0.0001])\n",
      "tensor([-0.0002])\n"
     ]
    }
   ],
   "source": [
    "print(module.weight.grad)\n",
    "print(module.bias.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fully-connected Networks for Image Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using this knowledge, you will create a neural network in PyTorch for image classification on the CIFAR-10 dataset. PyTorch uses the $DataLoader$ class for you to load data into batches to feed to your learning algorithms - we highly suggest you familiarze yourself with this as well as the Dataset API here: https://pytorch.org/docs/stable/data.html. Fill in the below code to instantiate 3 DataLoaders for your training, validation and test sets. We would prefer that you NOT use the `torchvision.transform` API - we want you to get some practice in data preprocessing! Here are the transformations we want you to perform:\n",
    "1. Split the `val_and_test_set` into two separate datasets (each with 5000 elements)\n",
    "2. Convert all the `np.array` elements into `torch.tensor` elements.\n",
    "3. All values will be pixel values in our images are in the range of [0, 256]. Normalize this so that each pixel is in the range [0, 1].\n",
    "3. Flatten all images. All your images will be of shape (32, 32, 3), we need them as flat (32 * 32 * 3) size tensors as input to our neural network.\n",
    "4. Load everything into a DataLoader. (check how this works in the PyTorch docs!) \n",
    "\n",
    "Be sure to have the options `shuffle=True` (so that your dataset is shuffled so that samples from the dataset are not correlated) and also `batch_size=32` or larger. This is a standard minibatch size. If you're curious about what batch size does (and are somewhat familiar with statistics), here's a great answer https://stats.stackexchange.com/questions/316464/how-does-batch-size-affect-convergence-of-sgd-and-why."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "from scipy.misc import imshow\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Documentation Transform\n",
    "\n",
    "## transform.ToTensor() :\n",
    "Converts a PIL Image or np.ndarray to a tensor\n",
    "Flattens to (CxHxW) = (32x32x3) (32x32 for each color (RBG))\n",
    "Transform data [0-255] to [0-1]\n",
    "\n",
    "## transform.Normalize()\n",
    "(Mean),(Standard Deviation)\n",
    "image = (image - mean) / std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "#transform = transforms.Compose([transforms.ToTensor(),transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "trainset = datasets.CIFAR10(root='./data', train=True, download=True) #50k Data\n",
    "val_and_test_set =datasets.CIFAR10(root='./data', train=False, download=True) #10k data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transform PIL Image to Np Arrays. Since we want to modify the original data, use np.asarray instead of np.array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transform into an array, divides by 255 to normalize. \n",
    "train = [(np.asarray(image)/255, target) for image,target in trainset]\n",
    "valtest = [(np.asarray(image)/255, target) for image,target in val_and_test_set]\n",
    "#train[0][0][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Note to self:\n",
    "Use Torch Command .view to reshape the inputs.\n",
    "We do not care of what our original interval may be, the weight does the job.\n",
    "Shuffle True --> Data reshuffled at each epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splits data into 2 equal set.\n",
    "val = valtest[:5000]\n",
    "test=valtest[5000:]\n",
    "#val[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transform all to torch.tensors\n",
    "TrainT=[(torch.tensor(image),torch.tensor(target)) for image, target in train]\n",
    "valT=[(torch.tensor(image),torch.tensor(target)) for image, target in val]\n",
    "testT=[(torch.tensor(image),torch.tensor(target)) for image, target in test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load Data to pytorch data loader. \n",
    "trainloader=torch.utils.data.DataLoader(TrainT, batch_size=32, shuffle=True)\n",
    "valloader=torch.utils.data.DataLoader(valT, batch_size=32, shuffle=True)\n",
    "testloader=torch.utils.data.DataLoader(testT, batch_size=32, shuffle=True)\n",
    "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAEICAYAAACQ6CLfAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAIABJREFUeJztnXuQXVd15r91H/1+qbvVUktqqSVZEjK2sY3QGOwBEh4GJylDASk8VQxVQ8aZCVSGSqYyFElNmEfVkKkJFH/MMCUCFZNheEyA4AEywWMMhoDtyC9Zsmy9392tZ6tf933X/HGvMu32/na31Orbcs73q+rq7r3uPmfdc8+65979nbWWuTuEEMkjtdwOCCGWBwW/EAlFwS9EQlHwC5FQFPxCJBQFvxAJRcEvFo2ZuZndtNx+iKtDwZ8QzOyYmb1zuf0QNw4KfgEzyyy3D6LxKPgTgJn9BYD1AP63mU2Z2R/UP6p/zMxOAPixmb3dzE7Nmff3nxbMLG1mnzazw2Y2aWZPm9lQYF/3mNlJM/uVhjw5cc0o+BOAu38EwAkAv+HuHQC+VTe9DcB2APcuYDO/B+ABAPcB6ALwzwDMzH6Amd0L4OsAPuDuj10f78VSoY97yeYz7j4NAGY232N/C8AfuPvL9f+fn2P/EIB/AeA+d3/hunoplgRd+ZPNyat47BCAwxH7JwF8S4H/2kHBnxxC6Zuzx6YBtF35x8zSAFbOsp8EsDmy/Q8BeJ+ZfXIxTorGoeBPDmMANkXsBwC0mNmvmVkWwB8BaJ5l/zMA/8HMtliN28ysb5b9DIB3APhdM/ud6+28uP4o+JPDfwLwR2Y2DuCDc43ufhnA76AW5KdR+yQwe/X/c6gtFP4IwASALwNonbONE6i9AfwbM/utJXgO4jpiKuYhRDLRlV+IhKLgFyKhKPiFSCgKfiESSkPv8Ovs6va+gVVBWzE/ExwHgHIxHxx353elZZtaqK2pmdvS2SZqS6XC+8vnpuicYiFHbV6pUJuBP7dUOs3npcLv5+0dnXROc+R4eKVMbbkcf83CtxUAVa/SGfkcP1aViB+xRWtmKpe5H9VqbHt8XibDwymT4a+ZI3wexNbiq8SN3EwOhUJx3ts1gUUGv5m9B8AXAKQB/Jm7fzb2+L6BVfjDz/23oO3US0/TeeeO7g+OVyrc/VXrX0dt6zdvp7YVq9dTW0treH8H9v2Czjl+aA+1lSb5m0Y68ty6VnRTW6alLTi+8+630jk3beXHKn/5IrXt2/sstVWrxeB4sRR+IweAF/fxmwMnxs9TW6FYoLZSMRx0Fy/wN66pGe5jucL3tXJlL7Wt6O2gtopPhvdVolOQz4XfGX7y2BN80hyu+WN//Q6w/wrgvQBuBvCAmd18rdsTQjSWxXzn3wngkLsfcfcigG8AuP/6uCWEWGoWE/xr8crEkFP1sVdgZg+a2W4z2z05cXkRuxNCXE8WE/yhRYVXfRFx913uvsPdd3R28e+qQojGspjgP4VamucV1qGW3CGEeA2wmNX+vwOwxcw2opYI8mEA/yQ2oVKpYOJSePW4r4evlPrKsDzomS46Z3A9T2CrVPkyaqrKV4GrM2G5KX/pAp3jOb5yvLZ/gNrWD/FiuEM3baC2NWvXBccHiMQKANlsM7WVe8LqAQAMrVvN55XDq/35PJfzxi9x9eP8ea46ZCKyLiy82r+ijz/nlnbu4+WJS9TW3MLDqepcqsxmwr5MXB6nc4qF8Gq/Mw0wwDUHv7uXzewTAP4GNanvK+6+71q3J4RoLIvS+d39hwB+eJ18EUI0EN3eK0RCUfALkVAU/EIkFAW/EAmlsXX73YFSWGYrFrj8NjMTlo2Gt77qhsK/Z2p6mtpiySW9/ZGkmWz4vXLLlq10zlvu2kFta1eFZTkA6O5eSW2lDM8GbGsJy0aZSIaYlSOZe9NcfiuQ1xIA2lrDEuGKHi5vbt7EU0P273+Z2mDcj0IhLN12d62gcyKJnbg8MUZtjvB5CsQzBS9dCp+ruRmeRMQy/q6mLJ+u/EIkFAW/EAlFwS9EQlHwC5FQFPxCJJSGrvZ7tYoySeywMl/Bbm5qDY5fPs9LO/Wt5ivp61/Pk2YGhtZQW5YtA0fqLZXKXFl4aYQnBM0cOce3meKryi+/MLd5bo03becr6W/d+SZqi60eT0TqM5w4Hk7wbMpGais28USt/pVc2Tlx8iDfJilrNpXjatDEBD+vMlleHq+riydBxeodsvKEsTqDzc3hc3H+Zsv/H135hUgoCn4hEoqCX4iEouAXIqEo+IVIKAp+IRJKw6W+wkxYYulo5RJQV284yeXON9xO5wxt2kJtk5FElpePnKS2iZmwXDM1zmutXRjnct7IKK8H1xVJ7EGKJ3x8/5vfDo5nf5O/z7/tzfdQWzbLZczVq7ksCg/LZeOXwt1pAOCZZ3l3o0ykzmB7J5cIy5WwVFmc4q9ZOnJJjHXlqVS4BHvhIpcPUwhLhLH2Xz094QS0dKQt2Kv3K4RIJAp+IRKKgl+IhKLgFyKhKPiFSCgKfiESSkOlPksZmpuzQVsp3Unn5Vo7guNHJ3hbped+/hS1XbzA69KdPsNrtGXT4ZSpbIpnXxVI2yoAyOe5bXAlf2nOjh6nti6S7TU5PkHnHDh6lPsx2E9t2Sz3cXAo3MprDRkHgBOjXGZ9+QVuGxjksuixE0RiK/HXrFrktkqkfmJLE5cjmzPh8x4AcvnwNru6uISZIS2+7Cqu54sKfjM7BmASQAVA2d15tUohxA3F9bjy/4o7uaNDCHHDou/8QiSUxQa/A/iRmT1tZg+GHmBmD5rZbjPbPT3Fv2sLIRrLYj/23+3uZ8xsAMAjZvaSuz8++wHuvgvALgBYt37DwjsKCCGWlEVd+d39TP33WQDfBbDzejglhFh6rvnKb2btAFLuPln/+90A/n1sTiqVQVvbqqDt7DjPtDt0MizzvLhvL99XRIaqRFqD5SZ5Ycc0kfRyBS6jjU9y22SkFdaxU/uprb2Vy6LbNm8LGyKS49/+7CfUtmHjRmrbuo23KevrC2edNbfw16W7i0tlqTIvFjpd4Ncw1vIqN86zCysVXnS1pZVLdlMTfJtdkczD5pZwJl6xGGthF84wrVa5TDmXxXzsXwXgu1YrF5oB8D/d/f8sYntCiAZyzcHv7kcAvOE6+iKEaCCS+oRIKAp+IRKKgl+IhKLgFyKhNDSrL53OoKc3nCV26OQBOm/kWDjrrC3LC1lenubFMacmzlKbRaSS8cmwNDee49JQhmQxAkD/qgFqa+0MS2UAsHaYr7MOEdno6PO/pHPSxmXAUoVnsZ07z4uT3nrr9uD4TVs20TlDkey8jrvuoLY9L52gtkI+XBi2kI1k9YHLclXnkvToaLg/IQA0NXMZs3sFOw+47JzLhTNaq75wqU9XfiESioJfiISi4BcioSj4hUgoCn4hEkpDV/sLhWkcPhyurffS4UN03pmRw8HxSiQJp7O7ndq2bRmmtlu230JtI+fCK6zHz3E/Vq4OJzIBwIbNPGmms48rAWOX+P78fFgZOXGcr4ifi7QU234zNeFdW8Mr+gAwPUVWo7l4AC9y1WHfE1yt2LKNt21btbYnOP7EU48HxwFgdIwnY5VKfLU/n+P+X4q0KWvtCPsYW7mfJm3vriaxR1d+IRKKgl+IhKLgFyKhKPiFSCgKfiESioJfiITSUKlvemoCTzz+SNiRVaT2HIDN228NjrdG2iptv3kLtW3buo7aKvlwYgwAeCosX02D9yzJZMOJJQCQToclHgAolXkiyPTkRWrrLoalqHKFF04+cZYnQbV0nOb76lpBbZs2DwfHPXK9yY2H69IBwEtPPkdtnuPnwS33vic4futtPMEot5tLfYcPHaO2trZwWzkA6O7po7Zaw6tXMzHBX5dCIXysXFKfEGI+FPxCJBQFvxAJRcEvREJR8AuRUBT8QiSUhkp9pWIZZ0+GZbE73vBrdF5zc7i2Wy9X5TC4htdhuxhp1XTyEJfRitWw/JYynqqWznDppeK8BiHKsXZjYckRALwS3l9Hd7h2IgBcmOJZgqkmnh1Z9VjfVWKLKFEdLfw1G14zRG0tae5HCuG6i7fewjMqe3q4BPtw7kfUNjrCpbm1A2uorWLhGpDZSMu5iYmwHLk/G25tF2LeK7+ZfcXMzprZ3lljvWb2iJkdrP/mgq8Q4oZkIR/7/xzA3DslPgXgUXffAuDR+v9CiNcQ8wa/uz8OYO5n4fsBPFT/+yEA77vOfgkhlphr/c6/yt1HAMDdR8yMlp0xswcBPAgA2SyvYS+EaCxLvtrv7rvcfYe778hkGrq+KISIcK3BP2ZmgwBQ/81b4Aghbkiu9VL8MICPAvhs/ff3FjIplcqgraM3aMtGVKPx8fB7S3Mvl2RmylxTyvPuWmhd0UltzVUjG+RSn0eOcL7Es9haWvnEVKS9VjUVntfRx6WmJufyZrqVCznexLXWqoWfm1W4dJhK8+ecbW+ittYObisXwrLuhdNjdE5fO28bdv9991Lb7uePUdtUpLhnvnAuOF4gLbkAoKczfO5n0hH9ew4Lkfq+DuCXALaZ2Skz+xhqQf8uMzsI4F31/4UQryHmvfK7+wPE9I7r7IsQooHo9l4hEoqCX4iEouAXIqEo+IVIKA2966apqRmD68PZVJbi70P5fDiDaWyCu9/Uw7PYSmUuDVnkLsTcVDhDrOTc90yGF+Isp7mtrYtnuA30jVObXwzLQ8VIjzmrcv9bW1upLRVRlaoe3l+lwmXRVDZSPDXNfZya5lmaRgpaNkfOt4lzXAZsbQtL1QDw1jffRm0vHz5ObXtfHA2OT03wbMsmUhi2Wo1lWr4SXfmFSCgKfiESioJfiISi4BcioSj4hUgoCn4hEkpDpT43wC0s55QiUtTMZFjKaY7IUJMTkUKceV44c2aCy0ZZktTX2c4lu5UruDTU1csz3Fb28OdWyXRTW645fBwvbuBZfYXKCLUhknlYKUeyC0kGZCXFsy0tIvX19PLswmol4iM5r7q7+fFtMi6XjU9GZNZSWAoGgNu3r6a2ns7w+fP97/NioefGwoVwy5E4mouu/EIkFAW/EAlFwS9EQlHwC5FQFPxCJJTGltN1B8gKcabKV467wzkMGOomy+8AXreJ1/fraOErvWnj74fTE+GV3vzMZTqntb1Ebdu2cCVgaMM6aktlN1Db1HjYx6HBQe7HUV5/tauXHHwAvSt48lEmE06eiuWdeCRRqKW9jdrKeb7CnSL7y8YSycDVoL7+DmqbmuGqw/R4OHkHANauDNcMfN9vvJvO+asf/N/geCZzHWv4CSH+YaLgFyKhKPiFSCgKfiESioJfiISi4BcioTRU6utsb8Pb3vzGoG3TzW+g886cPh0cX7uGS2Vbt2ymttUraVNhpJ3Lh5MkqaMQSX6xFN9eRztP7Ono4BJbuolLlVkimeamwy2hAODOW7h0OLx1mNpKVS5jOrmulKtclvM0P1bpLD9VS3muH1ZJoksqw6971sL9QGReocSPRybNa0NWiuHzamVEVrznH78pOP7Lp16gc+aykHZdXzGzs2a2d9bYZ8zstJk9V/+5b8F7FELcECzkY/+fA3hPYPzz7n57/eeH19ctIcRSM2/wu/vjAHhyvBDiNcliFvw+YWZ76l8LaKUFM3vQzHab2e6paV7sQAjRWK41+L8IYDOA2wGMAPhT9kB33+XuO9x9R0c7X8AQQjSWawp+dx9z94q7VwF8CcDO6+uWEGKpuSapz8wG3f1K4bf3A9gbe/wV2tpa8cbbXhe0vf4OLvXlbgnLdu3dPKuMV4oD3LiUk4pIMr3t4TpskW5d0XfXKmklBcxTiy0iKRUK4XZdm29aT+e0NnHJMTfNMxY9FTl9LGzzSH28qnNbJfKaxVpUFXPh41Gp8uecykTOj8grOnmBS77Hj56ktrvvuSM4PlPi9STbiBwZUZZfxbzBb2ZfB/B2AP1mdgrAHwN4u5ndDsABHAPw2wvfpRDiRmDe4Hf3BwLDX14CX4QQDUS39wqRUBT8QiQUBb8QCUXBL0RCaWhWXyqVQivJZOto4S2v2tuIm5FihbFCkRaT+mKSkoeluWqJS3Yx+coiRSTLEbEyJuc4KUDa0cMzIMsVvq9KNVIQkrTkAgBHJTieijlf4bZKhkuwjsiLTQrGWjXsHwA0R55ztsJfs/Y8n+djYckRAM4dGQuOr9vGi7ieT4Xvlr0aqU9XfiESioJfiISi4BcioSj4hUgoCn4hEoqCX4iE0lCpL51Oo7M7LDl5JJtuphCWa7zAe6oVyBwAmJ6aprZiic8rFMLZdOUyl8pKkQy8UmRfM5G+bzPTPNurTDIFO3u76ZzObt7XsKezn9pamsL9+ACgwnovWqSvHrits5MXNL1wlh/HfC4siVWrtP4MDPx5VSv8nOvq5HL1hvWrqC03Ez4fPVLstLszLJmnI/LxXHTlFyKhKPiFSCgKfiESioJfiISi4BcioTR0tX98fAJ/9fBfB22V7M/ovEuXwokPU5fP0zmpSK5HTAkYGwvvCwAqJFuoN9L+a0V/H7U1p/nhn74YbuEEAAcO7qe2ianw6vbQRt6SK53lSktXJ/d/40ZeF3DdULje4cZNa+mc3maeldLZwn2sRmo5Ih1OtilV+Ep6OtKSKx3xcdVwRBnp4kpAycNJRmkuOqC3N/ycM5Fkt7noyi9EQlHwC5FQFPxCJBQFvxAJRcEvREJR8AuRUBbSsWcIwFcBrEatC9Yud/+CmfUC+CaAYdS69vymu1+KbWticgqPPPaLoK1n3TY6zyth+erZXzxG52xYx+uf9fdx+er0qVFqK5O6b229PDGmmOJJP2OneAund+x8M7XdftvrqW2mkA+Op7L8pT564ji1HTh4mNpe2PsstfV0h5uyfuCD76dz7n79VmprivREWzc4RG1FIvVZpNhdrO5iidQmBIBUJlIXsIcnJrWSZJxqmkvSTPiMlKB8FQu58pcB/L67bwdwF4CPm9nNAD4F4FF33wLg0fr/QojXCPMGv7uPuPsz9b8nAewHsBbA/QAeqj/sIQDvWyonhRDXn6v6zm9mwwDuAPAkgFVXOvXWf/Pb3IQQNxwLDn4z6wDwbQCfdPeJq5j3oJntNrPdxSIvhCCEaCwLCn4zy6IW+F9z9+/Uh8fMbLBuHwRwNjTX3Xe5+w5339HUxO9vFkI0lnmD32rtbb4MYL+7f26W6WEAH63//VEA37v+7gkhloqFZPXdDeAjAF4ws+fqY58G8FkA3zKzjwE4AeBD821oRW8fPvTAPw3amge20Hkzk2H57eALz9M5g6u5/JOK1DlrbeEZYsVquOXS1lu47ysG+VLITD+vI/fr730ntbV1tlLbNJH6Ip21UCZtyAAgXw5vDwDOnr1IbcePngmOt7Xx4zt66gK1Hdt3kNpSee7jkdHgB1LsfPcOOmfD8Bpqi2UDploiaXhZLgMaq9VnfE6ThV+zq5H65g1+d/85ALbJdyx8V0KIGwnd4SdEQlHwC5FQFPxCJBQFvxAJRcEvREJpaAFPM6C5Kfx+c+ClvXTexOWw1Oex7Ksiz4iairTrsohW0tIczqUqzfD2WZfPcR/HTvCsvr/+m3ChUwC4NBnZ39Tl4HhnF5fYuleEW6gBQHuk8OSpU2E5DwAG+sOFOlu6uPT5sx/w53zx4B5qqxR5S7RDo+GCrKciLc+2bOfSbXdXG7et4C3RWtt4Vl93e/i8yrbwYpxtbeHXxX3hWp+u/EIkFAW/EAlFwS9EQlHwC5FQFPxCJBQFvxAJpaFSX7VcwuSFsGz34+/9gM47OXoqOJ4qhbPsAGDPnki9kYicVy7zrC2QTKpHvv9jOqUpy6Wy2++4k9qKTZ3UNlGYobYjJ8JZbBcu8P5+xTzP6jszeozajh7j29xxxxuD47/78d+jc5564pfUVr7MM/4mCrxITA5hqfXIbi6z/uzpEWprz3BZMdvEpbl0Mz8POonUt27DMJ1z/wc+HBwvlhd+PdeVX4iEouAXIqEo+IVIKAp+IRKKgl+IhNLQ1f5stgmDqwaDti3DG+k8R3g1OhNphZWOrOin0vw9z6s8EaeppT1syPKkjTVrwgkuAPD2e++lts62SAJJC6/99+LecF3DA4d4263Va4epLR9pk5Vu5T7uPfBScPzFAwfonLbh7dR25gx/zit6uG2gKVxXr62D10G8OMrbl104fYjazp0PJxEBQL4SSUIjBRZHxnl4vuUd4TllXvbvVejKL0RCUfALkVAU/EIkFAW/EAlFwS9EQlHwC5FQ5pX6zGwIwFcBrAZQBbDL3b9gZp8B8M8BnKs/9NPu/sPYtsrlMi6eC7d4uusfvYXOe8vb3hYcb27miRSZiJwXa9dVjbSuSiO8v1KR6yu5Ik/CuXDqKLVdzPMEkovneZusI0TSO3M2nFAFAB0DvD0VmrmMaU1c6iuWw8k2j/z053TOhs23UttQL5dMW1L8NG4jiVWFPK/hd2RiH7V1dPJaiBXnSWGjl6aorb9/ODg+U+Ln4o9/+lRwfHKS16ecy0J0/jKA33f3Z8ysE8DTZvZI3fZ5d/8vC96bEOKGYSG9+kYAjNT/njSz/QD427AQ4jXBVX3nN7NhAHcAeLI+9Akz22NmXzEzfpuVEOKGY8HBb2YdAL4N4JPuPgHgiwA2A7gdtU8Gf0rmPWhmu81s9+QU/54lhGgsCwp+M8uiFvhfc/fvAIC7j7l7xd2rAL4EYGdorrvvcvcd7r6js4NXpxFCNJZ5g99qLWy+DGC/u39u1vjsDJ33A+Atd4QQNxwLWe2/G8BHALxgZs/Vxz4N4AEzux2AAzgG4Lfn21AqZWgnbYYuTOTpvGf3PB0cHxjgywyrBvqprVTiMtqlS+PUhnzYx0yVb2/tRi6jDa3gn4ROH+B15KaneM26gVWrg+NtfT10TrqFy1czOf66DA6up7bRM+G6i+cvhNuJAcDgmkgbtUhrtqkCP/7IhM+3UpXLs82tJHsTQHMkW7R44Ry1IRWu0wcAq0hWZbHAW86xw8GP0qtZyGr/zwGEnnFU0xdC3NjoDj8hEoqCX4iEouAXIqEo+IVIKAp+IRJKQwt4pgxozoYzlQp5LrH94hePBse9xGWorjZeoLFU4tlX+RxvAZYh75UbhofonFvuupnaNq/nMuD4ybBUBgCjl85TW1NrWNra3BeWAAHg3DmecXbrtluo7fW3bqO2b/yPrwbHMwgX1ASA0jR/PYtFbvNY1cqW8Gsda581vHETtZ09+TLfV4pnmba28/1t3741OJ6f4a/L0OBAcPynTVxSnIuu/EIkFAW/EAlFwS9EQlHwC5FQFPxCJBQFvxAJpaFSX7VaxUyOFLSMFNW8972/Ht5ekWeBpSNyXrXCCyN6mss16UxYpmpp54UsR8e5dDg5zvvWXcxx/62FF9V8+bkjwfELv+QZZ5s2csnuTTdtobZiJOOvtSksbXkkozKWQZhK81OVtLoDAOSqpM9jhR/fDeu41JefukBtN3fxbMCnnn6W2s4cD8uHuWl+fvvMpeB4scAzPueiK78QCUXBL0RCUfALkVAU/EIkFAW/EAlFwS9EQmlsVl/K0N4Rlsu6I5UHO1eGs54KEVmjJfK+1mQ8s8xbeTZgc1t4XjXPs68mJyeoLd3GC2cObOYFNze38ay+g0fDvfpgXMLMkqKqAHB65AS19fXzAqrMVsxx+apQ4MU9pyMZf4VI9lupEJaWMy1cnl21ZiW1HR8Zo7axE+TYA8hP8ed2eN9zwfG+Pu6Hr+gNj0cKnc5FV34hEoqCX4iEouAXIqEo+IVIKAp+IRLKvKv9ZtYC4HEAzfXH/6W7/7GZbQTwDQC9AJ4B8BF35/2FAFSrecxMkmSWKn8fylpHcHxsjK+gHnzxGLW1ZPiKflM3X2XvJ+3B1vR30zmZSMJSX3cftUVyj5DPhZM6AGBgIKwgrF0TXh0GgJHRUWo7cGA/tQ0XN1IbU2ImJ/lrNjPDV9InLnPVJLbaXymGE6vSzTwJZ99e3uot1kJrYGAVta29jddCHFgZnte/ktddbCH+P/q3j9E5c1nIlb8A4Ffd/Q2oteN+j5ndBeBPAHze3bcAuATgYwveqxBi2Zk3+L3GlbfWbP3HAfwqgL+sjz8E4H1L4qEQYklY0Hd+M0vXO/SeBfAIgMMAxt39SlL0KQBrl8ZFIcRSsKDgd/eKu98OYB2AnQC2hx4WmmtmD5rZbjPbPTlJCnkIIRrOVa32u/s4gJ8AuAtAj5ldWTBcB+AMmbPL3Xe4+47OTn5LpRCiscwb/Ga20sx66n+3AngngP0AHgPwwfrDPgrge0vlpBDi+rOQxJ5BAA+ZWRq1N4tvufv3zexFAN8ws/8I4FkAX553S1VHlbRdSkXehzKlcFJKF2n9BQBPP/FTahsd44kxluVJLjt3vjE4fs+bd9A5ly9zaWvPM09S23SeJ7IcOHGS2o4cOxYcz83wr1zuvAheSxdPLpmYmKS2SdJSbHqCy5SRUnzIpLm1O/KJcs3GsBy5om+QzhlYwyW2NXfcSm29kRp+TbHakMwWScaCh+MlFWkZNpd5g9/d9wC4IzB+BLXv/0KI1yC6w0+IhKLgFyKhKPiFSCgKfiESioJfiIRiV1Pza9E7MzsH4Hj9334AXHNrHPLjlciPV/Ja82ODu3N9dhYNDf5X7Nhst7tzgVx+yA/5saR+6GO/EAlFwS9EQlnO4N+1jPuejfx4JfLjlfyD9WPZvvMLIZYXfewXIqEo+IVIKMsS/Gb2HjN72cwOmdmnlsOHuh/HzOwFM3vOzHY3cL9fMbOzZrZ31livmT1iZgfrv3kjvKX14zNmdrp+TJ4zs/sa4MeQmT1mZvvNbJ+Z/av6eEOPScSPhh4TM2sxs6fM7Pm6H/+uPr7RzJ6sH49vmkWaTi4Ed2/oD4A0ajUANwFoAvA8gJsb7Ufdl2MA+pdhv28FcCeAvbPG/jOAT9X//hSAP1kmPz4D4F83+HgMAriz/ncngAMAbm70MYn40dBjglppg47631kAT6JWPetbAD5cH//vAP7lYvazHFf+nQAOufsRr9X5/waA+5fBj2XD3R8HcHHO8P2oVUEGGlQNmfjRcNx9xN2fqf89iVqlqLVo8DH0QAcWAAABvUlEQVSJ+NFQvMaSV8xejuBfC2B2KZrlrPzrAH5kZk+b2YPL5MMVVrn7CFA7CQEMLKMvnzCzPfWvBUv+9WM2ZjaMWvGYJ7GMx2SOH0CDj0kjKmYvR/CH6jEtl954t7vfCeC9AD5uZm9dJj9uJL4IYDNqDVpGAPxpo3ZsZh0Avg3gk+7OW/Q03o+GHxNfRMXshbIcwX8KwNCs/2nl36XG3c/Uf58F8F0sb1myMTMbBID677PL4YS7j9VPvCqAL6FBx8TMsqgF3Nfc/Tv14YYfk5Afy3VM6vu+6orZC2U5gv/vAGypr1w2AfgwgIcb7YSZtZtZ55W/AbwbwN74rCXlYdSqIAPLWA35SrDVeT8acEzMzFArALvf3T83y9TQY8L8aPQxaVjF7EatYM5ZzbwPtZXUwwD+cJl82ISa0vA8gH2N9APA11H7+FhC7ZPQxwD0AXgUwMH6795l8uMvALwAYA9qwTfYAD/uQe0j7B4Az9V/7mv0MYn40dBjAuA21Cpi70HtjebfzjpnnwJwCMD/AtC8mP3o9l4hEoru8BMioSj4hUgoCn4hEoqCX4iEouAXIqEo+IVIKAp+IRLK/wOns+q6/Wp9LQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for x,y in TrainT[:3]:\n",
    "    plt.title(classes[y])\n",
    "    plt.imshow(x)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Whatever is under this point is left to demistefy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(Net, self).__init__()                    # Inherited from the parent class nn.Module\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)  # 1st Full-Connected Layer: 784 (input data) -> 500 (hidden node)\n",
    "        self.relu = nn.ReLU()                          # Non-Linear ReLU Layer: max(0,x)\n",
    "        self.fc2 = nn.Linear(hidden_size, num_classes) # 2nd Full-Connected Layer: 500 (hidden node) -> 10 (output class)\n",
    "    \n",
    "    def forward(self, x):                              # Forward pass: stacking each layer together\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 50\n",
    "LEARNING_RATE = 0.001\n",
    "INPUT_SIZE = 32*32*3\n",
    "HIDDEN_SIZE = 500\n",
    "OUTPUT_SIZE = 10\n",
    "\n",
    "net = Net(INPUT_SIZE, HIDDEN_SIZE, OUTPUT_SIZE)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(epoch, train_loss, val_acc) = (0, 1.8508871861626839, 0.4030652866242038)\n",
      "(epoch, train_loss, val_acc) = (1, 1.6885500649039134, 0.41162420382165604)\n",
      "(epoch, train_loss, val_acc) = (2, 1.6208200575217786, 0.44705414012738853)\n",
      "(epoch, train_loss, val_acc) = (3, 1.5729778833062849, 0.4578025477707006)\n",
      "(epoch, train_loss, val_acc) = (4, 1.5452826585971005, 0.4609872611464968)\n",
      "(epoch, train_loss, val_acc) = (5, 1.5205885940305865, 0.4482484076433121)\n",
      "(epoch, train_loss, val_acc) = (6, 1.5058570129323754, 0.46894904458598724)\n",
      "(epoch, train_loss, val_acc) = (7, 1.4879329640439742, 0.44227707006369427)\n",
      "(epoch, train_loss, val_acc) = (8, 1.4746476870962084, 0.4635748407643312)\n",
      "(epoch, train_loss, val_acc) = (9, 1.458981120960116, 0.46636146496815284)\n",
      "(epoch, train_loss, val_acc) = (10, 1.4443499500447943, 0.4653662420382166)\n",
      "(epoch, train_loss, val_acc) = (11, 1.4370700250202773, 0.4898487261146497)\n",
      "(epoch, train_loss, val_acc) = (12, 1.4212125585922735, 0.4888535031847134)\n",
      "(epoch, train_loss, val_acc) = (13, 1.4142744529727782, 0.4745222929936306)\n",
      "(epoch, train_loss, val_acc) = (14, 1.4024082573651504, 0.47113853503184716)\n",
      "(epoch, train_loss, val_acc) = (15, 1.397199984971179, 0.4856687898089172)\n",
      "(epoch, train_loss, val_acc) = (16, 1.3896316723112716, 0.48168789808917195)\n",
      "(epoch, train_loss, val_acc) = (17, 1.383870784434003, 0.46337579617834396)\n",
      "(epoch, train_loss, val_acc) = (18, 1.3762437373647611, 0.48586783439490444)\n",
      "(epoch, train_loss, val_acc) = (19, 1.366991334173516, 0.473328025477707)\n",
      "(epoch, train_loss, val_acc) = (20, 1.363153702123609, 0.47093949044585987)\n",
      "(epoch, train_loss, val_acc) = (21, 1.3593960596595296, 0.46476910828025475)\n",
      "(epoch, train_loss, val_acc) = (22, 1.3501944925566933, 0.48427547770700635)\n",
      "(epoch, train_loss, val_acc) = (23, 1.3468673320359628, 0.48865445859872614)\n",
      "(epoch, train_loss, val_acc) = (24, 1.3396692200494134, 0.4896496815286624)\n",
      "(epoch, train_loss, val_acc) = (25, 1.332930658081748, 0.49442675159235666)\n",
      "(epoch, train_loss, val_acc) = (26, 1.3303885821417518, 0.4850716560509554)\n",
      "(epoch, train_loss, val_acc) = (27, 1.3234329721672902, 0.48487261146496813)\n",
      "(epoch, train_loss, val_acc) = (28, 1.316765857368269, 0.47312898089171973)\n",
      "(epoch, train_loss, val_acc) = (29, 1.3139673783011874, 0.48527070063694266)\n",
      "(epoch, train_loss, val_acc) = (30, 1.311039626445819, 0.4737261146496815)\n",
      "(epoch, train_loss, val_acc) = (31, 1.3094620787784677, 0.49104299363057324)\n",
      "(epoch, train_loss, val_acc) = (32, 1.3034967387325094, 0.49562101910828027)\n",
      "(epoch, train_loss, val_acc) = (33, 1.2983771606812626, 0.4876592356687898)\n",
      "(epoch, train_loss, val_acc) = (34, 1.2943629915334443, 0.5011942675159236)\n",
      "(epoch, train_loss, val_acc) = (35, 1.2905325922764652, 0.48288216560509556)\n",
      "(epoch, train_loss, val_acc) = (36, 1.2869517485910855, 0.4898487261146497)\n",
      "(epoch, train_loss, val_acc) = (37, 1.2852720704813914, 0.4713375796178344)\n",
      "(epoch, train_loss, val_acc) = (38, 1.276415530146503, 0.48785828025477707)\n",
      "(epoch, train_loss, val_acc) = (39, 1.2780367987550991, 0.4745222929936306)\n",
      "(epoch, train_loss, val_acc) = (40, 1.271210126173626, 0.48546974522292996)\n",
      "(epoch, train_loss, val_acc) = (41, 1.269397649258585, 0.48427547770700635)\n",
      "(epoch, train_loss, val_acc) = (42, 1.2663241268653407, 0.48527070063694266)\n",
      "(epoch, train_loss, val_acc) = (43, 1.2609865363408417, 0.477109872611465)\n",
      "(epoch, train_loss, val_acc) = (44, 1.2599831993428852, 0.4906449044585987)\n",
      "(epoch, train_loss, val_acc) = (45, 1.25854207091963, 0.48427547770700635)\n",
      "(epoch, train_loss, val_acc) = (46, 1.2578364281568937, 0.49203821656050956)\n",
      "(epoch, train_loss, val_acc) = (47, 1.2484868653523793, 0.4753184713375796)\n",
      "(epoch, train_loss, val_acc) = (48, 1.2505438470413344, 0.495421974522293)\n",
      "(epoch, train_loss, val_acc) = (49, 1.2455731616978147, 0.4918391719745223)\n",
      "Wall time: 1h 51min 43s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for epoch in range(EPOCHS):\n",
    "    \n",
    "    total_loss = 0\n",
    "    \n",
    "    for images, labels in trainloader:\n",
    "                \n",
    "        ### YOUR CODE HERE - Zero gradients, call .backward(), and step the optimizer.\n",
    "        images=images.view(images.shape[0],-1).float()\n",
    "        images.require_grad=True\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(images)\n",
    "        loss = loss_fn(outputs,labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "                \n",
    "        total_loss += loss.item()\n",
    "        \n",
    "    average_loss = total_loss / len(trainloader)\n",
    "    \n",
    "    ### Calculate validation accuracy here by iterating through the validation set.\n",
    "    ### We use torch.no_grad() here because we don't want to accumulate gradients in our function.\n",
    "    with torch.no_grad():\n",
    "        correction=0.0\n",
    "        for images,labels in valloader:\n",
    "            images=images.view(images.shape[0],-1).float()\n",
    "            val_output = net(images)\n",
    "            val_output = torch.argmax(val_output.reshape(val_output.shape[0], -1), dim=1)\n",
    "            correction += torch.sum(val_output == labels).item() / float(val_output.shape[0])\n",
    "        val_acc=correction/len(valloader)\n",
    "    print(\"(epoch, train_loss, val_acc) = ({0}, {1}, {2})\".format(epoch, average_loss, val_acc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.4755175159235669\n",
      "Wall time: 4.25 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "with torch.no_grad():\n",
    "    correction=0.0\n",
    "    for images,labels in testloader:\n",
    "        images=images.view(images.shape[0],-1).float()\n",
    "        test_output = net(images)\n",
    "        test_output = torch.argmax(test_output.reshape(test_output.shape[0], -1), dim=1)\n",
    "        correction += torch.sum(test_output == labels).item() / float(test_output.shape[0])\n",
    "    test_acc=correction/len(testloader)\n",
    "    print(\"Test accuracy:\", test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End of assignment. I still have a bug in my code downthere. Will try to keep on debugging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Questions:\n",
    "\n",
    "1. In the given example, what if we wanted to make our target value 0, what would we do? loss = 0?\n",
    "1. Why are we dividing by 256 and not 255?\n",
    "3. What is the num_workers in pytorch.DataLoader? What are sub-processes?\n",
    "4. Is it normal it takes that long?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "https://colab.research.google.com/drive/1jxUPzMsAkBboHMQtGyfv5M5c7hU8Ss2c#scrollTo=qBrGa7qMJKcB\n",
    "https://github.com/pytorch/tutorials/blob/master/beginner_source/blitz/cifar10_tutorial.py#L154"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now I think I understood, but please correct me if I am wrong. torchvision.datasets is image, label. The image is a 3D array, with 32 rows, 32 columns, 3 depth? and the labe is one of the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#trainset = [(np.asarray(image) / 256, label) for image, label in trainset] #This normalizes the data to [0,256]\n",
    "#val_and_test_set = [(np.asarray(image) / 256, label) for image, label in trainset] #This normalizes the data to [0,256]\n",
    "#val=val_and_test_set[:5000] #0-4999 are valset\n",
    "#test=val_and_test_set[5000:]#5000-9999 are test set.\n",
    "\n",
    "#trainset=[(torch.tensor(image.reshape(1,-1)), torch.tensor(label)) for image,label in trainset] #reshapes, -1 means TBD size\n",
    "#val=[(torch.tensor(image.reshape(1,-1)),torch.tensor(label)) for image, label in val]\n",
    "#test=[(torch.tensor(image.reshape(1,-1)), torch.tensor(label)) for image,label in test]\n",
    "### YOUR CODE HERE\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(trainset, shuffle=True, batch_size=32)\n",
    "valloader = torch.utils.data.DataLoader(val_and_test_set, shuffle=True, batch_size=32)\n",
    "#testloader = torch.utils.data.DataLoader(test, shuffle=True, batch_size=32)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CIFAR-10 consists of 32 x 32 color images, each corresponding to a unique class indicating the object present within the image. Use Matplotlib to print out the first few images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for image, label in trainset[:4]:\n",
    "    x=image\n",
    "    plt.title(classes[label])\n",
    "    plt.imshow(x.reshape(32,32,-1))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now try to build and train a plain neural network that properly classifies images in the CIFAR-10 dataset. Try to achieve at least around 40% accuracy (the higher the better!).\n",
    "\n",
    "Take a look at the PyTorch documentation for some help in how to do this.\n",
    "\n",
    "Google is your friend -- Looking things up on the PyTorch docs and on StackOverflow will be helpful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/pytorch/tutorials/blob/master/beginner_source/blitz/cifar10_tutorial.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(input_dim, hidden_dim, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(hidden_dim, output_dim, 5)\n",
    "        self.fc1 = nn.Linear(output_dim*5*5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, output_dim * 5 * 5)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 16 * 5 * 5)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "net = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv1): Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=400, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      ")\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "dimension specified as 0 but tensor has no dimensions",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-31-abde246bbdc0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     26\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    476\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 477\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    478\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\loss.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m    860\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    861\u001b[0m         return F.cross_entropy(input, target, weight=self.weight,\n\u001b[1;32m--> 862\u001b[1;33m                                ignore_index=self.ignore_index, reduction=self.reduction)\n\u001b[0m\u001b[0;32m    863\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    864\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[1;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[0;32m   1548\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1549\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1550\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mnll_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1551\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1552\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mnll_loss\u001b[1;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[0;32m   1401\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Expected 2 or more dimensions (got {})'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1402\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1403\u001b[1;33m     \u001b[1;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1404\u001b[0m         raise ValueError('Expected input batch_size ({}) to match target batch_size ({}).'\n\u001b[0;32m   1405\u001b[0m                          .format(input.size(0), target.size(0)))\n",
      "\u001b[1;31mRuntimeError\u001b[0m: dimension specified as 0 but tensor has no dimensions"
     ]
    }
   ],
   "source": [
    "EPOCHS = 10\n",
    "LEARNING_RATE = 0.01\n",
    "INPUT_SIZE = 32*32*3\n",
    "HIDDEN_SIZE = 5\n",
    "\n",
    "#are there specfic values for the above? or is it just purely random\n",
    "\n",
    "OUTPUT_SIZE = 10\n",
    "\n",
    "#net = Net(INPUT_SIZE, HIDDEN_SIZE, OUTPUT_SIZE)\n",
    "net=Net()\n",
    "### Define an optimizer and a loss function here. We pass our network parameters to our optimizer here so we know\n",
    "### which values to update by how much.\n",
    "optimizer = optim.Adam(net.parameters(),lr=LEARNING_RATE)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "print(net)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    \n",
    "    total_loss = 0\n",
    "    \n",
    "    for images, labels in trainloader:\n",
    "                \n",
    "        ### YOUR CODE HERE - Zero gradients, call .backward(), and step the optimizer.\\\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(images)\n",
    "        loss = loss_fn(outputs, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    ### Calculate validation accuracy here by iterating through the validation set.\n",
    "    ### We use torch.no_grad() here because we don't want to accumulate gradients in our function.\n",
    "    with torch.no_grad():\n",
    "        for images, labels in valloader:\n",
    "            outputs = net(images)\n",
    "            predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "        val_acc = 100*correct/total\n",
    "    \n",
    "    print(\"(epoch, train_loss, val_acc) = ({0}, {1}, {2})\".format(epoch, average_loss, val_acc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE HERE - Here, we test the overall accuracy of our model.\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        outputs = net(images)\n",
    "        predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "    print(\"Test accuracy:\", test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission\n",
    "For submiting, please download this notebook as a `.py` file. To do so, click on `File -> Download as -> Python (.py)`. Put the downloaded `assignment3.py` into this folder and commit the file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Resources\n",
    "If you're interested in using PyTorch as a framework for deep learning (especially for your final projects! We highly recommend you use this!), check out the PyTorch tutorials: https://pytorch.org/tutorials/. They have tutorials for everything from image to text to reinforcement learning tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
