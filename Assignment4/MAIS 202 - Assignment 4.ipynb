{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 4: Mid Point Review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations, you have reached the midpoint of the course! The rest of the course will be focused on more advanced state of the art machine learning techniques. However, before we dive into that, this assignment will be focused on ensuring you understand all the core concepts that have been covered so far. Keep in mind, these are questions that can be asked during machine learning internship interviews, so do make sure you understand them if you want to dive into this industry!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Linear vs Polynomial Regression\n",
    "- Describe both Linear Regression and Polynomial Regression (3 lines or less each).\n",
    "\n",
    "- Describe overfitting vs underfitting with respect to parameters.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## YOUR ANSWER HERE - YOUR MAY USE MARKDOWN, LATEX, CODE, DIAGRAMS, ETC\n",
    "\n",
    "# Linear regression is fitting your data to a line (think like, y = mx + b). Polynomial is more \"shapeable\" and is in the\n",
    "# form y = ax + bx^2 + cx^3 ..... zx^n.  Note if it's too high of a dregree you can overfit. You can make models to make\n",
    "# predictions on your line.\n",
    "\n",
    "\n",
    "# Overfitting is when you \"memorize\" your training set by fitting too well to it. Your model captures the noise in your data\n",
    "# which actually results in a worse overall model that doesn't predict well with your testing set.\n",
    "# See image: http://www.frank-dieterle.de/phd/images/image024.gif"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 2) Logistic Regression vs. Linear SVM\n",
    "- Describe how logistic regression works (3 lines or less)\n",
    "- Describe how linear SVM works. Mention the role(s) of:\n",
    "    - support vectors\n",
    "    - margin\n",
    "    - slack variables\n",
    "    - kernels\n",
    "- Plot an example for SVM where the linear kernel is not enough to separate the data, but another kernel works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## YOUR ANSWER HERE - YOUR MAY USE MARKDOWN, LATEX, CODE, DIAGRAMS, ETC\n",
    "\n",
    "# Logistic regression takes a value and spits out a value that you can round to 0 to 1. Good for binary classification.\n",
    "\n",
    "# Support Vector Machines: help you cluster and classify data by expanding them splitting them in euclidean space\n",
    "# Vectors: Each point is represented by a vector (think of MATH 133 linalg) and you perform transformations on those vect.\n",
    "# Margin: You want to maximize the margin on either side of your splitting vector or plane. There's a tradeoff between\n",
    "#           large margin and number of mistakes in training.\n",
    "# Slack Variables: Related to margin. Depending on value, softens the borders to determine between margin violations \n",
    "#           and mistakes in classification\n",
    "# Kernels: Math tools that expand your datapoints into a greater dimensional space so you can split them easier (see ex)\n",
    "\n",
    "# Ex: http://i2.wp.com/blog.hackerearth.com/wp-content/uploads/2017/02/kernel.png?resize=730%2C291"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Linear SVM vs k-NN\n",
    "- K-Nearest Neighbours is a popular unsupervised learning algorithm. Explain the difference between supervised and unsupervised learning?\n",
    "- Explain the \"training\" and \"testing\" phase of a k-NN model. List out the key steps.\n",
    "- Plot a example dataset which works in an SVM classification and not k-NN classification. Repeat for the reverse scenario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## YOUR ANSWER HERE - YOUR MAY USE MARKDOWN, LATEX, CODE, DIAGRAMS, ETC\n",
    "\n",
    "#\t- Supervised: regression vs classification\n",
    "#\t\t○ Continuous output (ex: predicting sales price) vs categories (cats vs dogs).\n",
    "#\t- Unsupervised: \n",
    "#\t\t○ Learn patterns within outputs using CLUSTERING and math (you can define similarity based on what you want)\n",
    "#\t\t○ Ex: imaging. Like colours or medical imaging (like brain tissue pixels vs brain tumour pixels)\n",
    "\n",
    "\n",
    "# K-NN relies on classification based on on the class of the nearest neighbours. It does not really have a \"model\" to train.\n",
    "# For testing make sure to use K-cross validation because the neighbours are pretty damn important for this type of\n",
    "# classification (so you have the appropriate density of the points in the different classes)\n",
    "\n",
    "# K-NN needs a lot of points, so a dataset with not a lot of points would need SVM\n",
    "# Ex: this would work BETTER with SVM : https://eight2late.files.wordpress.com/2016/12/svm-fig-1.png\n",
    "\n",
    "# K-NN is good for multi class but SVM not so much (you have to change it a bunch)\n",
    "# Ex: good for K-NN: https://i.stack.imgur.com/Y4gxj.png\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) k-NN Implementation\n",
    "- Implement the \"training\" phase of a k-NN algorithm by hand (ie. Don't use the sklearn implementation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)  \\\n",
      "142                5.8               2.7                5.1               1.9   \n",
      "14                 5.8               4.0                1.2               0.2   \n",
      "82                 5.8               2.7                3.9               1.2   \n",
      "128                6.4               2.8                5.6               2.1   \n",
      "29                 4.7               3.2                1.6               0.2   \n",
      "53                 5.5               2.3                4.0               1.3   \n",
      "85                 6.0               3.4                4.5               1.6   \n",
      "143                6.8               3.2                5.9               2.3   \n",
      "75                 6.6               3.0                4.4               1.4   \n",
      "108                6.7               2.5                5.8               1.8   \n",
      "132                6.4               2.8                5.6               2.2   \n",
      "81                 5.5               2.4                3.7               1.0   \n",
      "134                6.1               2.6                5.6               1.4   \n",
      "118                7.7               2.6                6.9               2.3   \n",
      "41                 4.5               2.3                1.3               0.3   \n",
      "123                6.3               2.7                4.9               1.8   \n",
      "67                 5.8               2.7                4.1               1.0   \n",
      "119                6.0               2.2                5.0               1.5   \n",
      "57                 4.9               2.4                3.3               1.0   \n",
      "130                7.4               2.8                6.1               1.9   \n",
      "2                  4.7               3.2                1.3               0.2   \n",
      "109                7.2               3.6                6.1               2.5   \n",
      "3                  4.6               3.1                1.5               0.2   \n",
      "25                 5.0               3.0                1.6               0.2   \n",
      "37                 4.9               3.6                1.4               0.1   \n",
      "51                 6.4               3.2                4.5               1.5   \n",
      "103                6.3               2.9                5.6               1.8   \n",
      "87                 6.3               2.3                4.4               1.3   \n",
      "60                 5.0               2.0                3.5               1.0   \n",
      "147                6.5               3.0                5.2               2.0   \n",
      "..                 ...               ...                ...               ...   \n",
      "52                 6.9               3.1                4.9               1.5   \n",
      "45                 4.8               3.0                1.4               0.3   \n",
      "12                 4.8               3.0                1.4               0.1   \n",
      "89                 5.5               2.5                4.0               1.3   \n",
      "86                 6.7               3.1                4.7               1.5   \n",
      "83                 6.0               2.7                5.1               1.6   \n",
      "146                6.3               2.5                5.0               1.9   \n",
      "101                5.8               2.7                5.1               1.9   \n",
      "46                 5.1               3.8                1.6               0.2   \n",
      "93                 5.0               2.3                3.3               1.0   \n",
      "38                 4.4               3.0                1.3               0.2   \n",
      "4                  5.0               3.6                1.4               0.2   \n",
      "137                6.4               3.1                5.5               1.8   \n",
      "10                 5.4               3.7                1.5               0.2   \n",
      "115                6.4               3.2                5.3               2.3   \n",
      "77                 6.7               3.0                5.0               1.7   \n",
      "120                6.9               3.2                5.7               2.3   \n",
      "35                 5.0               3.2                1.2               0.2   \n",
      "145                6.7               3.0                5.2               2.3   \n",
      "5                  5.4               3.9                1.7               0.4   \n",
      "112                6.8               3.0                5.5               2.1   \n",
      "47                 4.6               3.2                1.4               0.2   \n",
      "124                6.7               3.3                5.7               2.1   \n",
      "125                7.2               3.2                6.0               1.8   \n",
      "49                 5.0               3.3                1.4               0.2   \n",
      "96                 5.7               2.9                4.2               1.3   \n",
      "105                7.6               3.0                6.6               2.1   \n",
      "131                7.9               3.8                6.4               2.0   \n",
      "11                 4.8               3.4                1.6               0.2   \n",
      "110                6.5               3.2                5.1               2.0   \n",
      "\n",
      "     target  \n",
      "142     2.0  \n",
      "14      0.0  \n",
      "82      1.0  \n",
      "128     2.0  \n",
      "29      0.0  \n",
      "53      1.0  \n",
      "85      1.0  \n",
      "143     2.0  \n",
      "75      1.0  \n",
      "108     2.0  \n",
      "132     2.0  \n",
      "81      1.0  \n",
      "134     2.0  \n",
      "118     2.0  \n",
      "41      0.0  \n",
      "123     2.0  \n",
      "67      1.0  \n",
      "119     2.0  \n",
      "57      1.0  \n",
      "130     2.0  \n",
      "2       0.0  \n",
      "109     2.0  \n",
      "3       0.0  \n",
      "25      0.0  \n",
      "37      0.0  \n",
      "51      1.0  \n",
      "103     2.0  \n",
      "87      1.0  \n",
      "60      1.0  \n",
      "147     2.0  \n",
      "..      ...  \n",
      "52      1.0  \n",
      "45      0.0  \n",
      "12      0.0  \n",
      "89      1.0  \n",
      "86      1.0  \n",
      "83      1.0  \n",
      "146     2.0  \n",
      "101     2.0  \n",
      "46      0.0  \n",
      "93      1.0  \n",
      "38      0.0  \n",
      "4       0.0  \n",
      "137     2.0  \n",
      "10      0.0  \n",
      "115     2.0  \n",
      "77      1.0  \n",
      "120     2.0  \n",
      "35      0.0  \n",
      "145     2.0  \n",
      "5       0.0  \n",
      "112     2.0  \n",
      "47      0.0  \n",
      "124     2.0  \n",
      "125     2.0  \n",
      "49      0.0  \n",
      "96      1.0  \n",
      "105     2.0  \n",
      "131     2.0  \n",
      "11      0.0  \n",
      "110     2.0  \n",
      "\n",
      "[150 rows x 5 columns]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'slice'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-e8a6183e84e7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m \u001b[0mtrainingSet\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtestSet\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0miris_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0miris_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   2686\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2687\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2688\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_column\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2689\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2690\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_getitem_column\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m_getitem_column\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   2693\u001b[0m         \u001b[1;31m# get column\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2694\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_unique\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2695\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_item_cache\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2696\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2697\u001b[0m         \u001b[1;31m# duplicate columns & possible reduce dimensionality\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m_get_item_cache\u001b[1;34m(self, item)\u001b[0m\n\u001b[0;32m   2485\u001b[0m         \u001b[1;34m\"\"\"Return the cached item, item represents a label indexer.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2486\u001b[0m         \u001b[0mcache\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_item_cache\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2487\u001b[1;33m         \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2488\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mres\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2489\u001b[0m             \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: unhashable type: 'slice'"
     ]
    }
   ],
   "source": [
    "# Implement kNN by hand. It might be useful to store all distances in one array/list\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "import math\n",
    "\n",
    "# loading dataset\n",
    "iris = load_iris()\n",
    "iris_df = pd.DataFrame(data= np.c_[iris['data'], iris['target']],\n",
    "                     columns= iris['feature_names'] + ['target'])\n",
    "\n",
    "# Preview dataset\n",
    "iris_df.head()\n",
    "from sklearn.utils import shuffle\n",
    "iris_df = shuffle(iris_df)\n",
    "print(iris_df)\n",
    "\n",
    "\n",
    "## YOUR CODE HERE\n",
    "trainingSet = pd.DataFrame()\n",
    "testSet = pd.DataFrame()\n",
    "trainingSet, testSet = iris_df[:100, :], iris_df[100:, :] # I chose a 2/3 split\n",
    "\n",
    "\n",
    "def euclideanDistance(instance1, instance2, length):\n",
    "    distance = 0\n",
    "    for x in range(length):\n",
    "        distance += pow((instance1[x] - instance2[x]), 2)\n",
    "    return math.sqrt(distance)\n",
    "\n",
    "########### TBH BEYOND THIS IM STUCK SO HERE'S THE CLEAREST ONE FROM ONE OF THE VARIOUS TUTORIALS I FOUND ###########\n",
    "##### SO I KNOW FOR THIS MODEL IT TAKES THE EUCLIDEAN DISTANCE\n",
    "##### FROM NEIGHBOURS AND GETS CONSENSUS BASED ON THEIR CLASS (TAKES VOTES) TO GUESS WHAT THE CLASS OF IT IS AS WELL\n",
    "##### AND MAKE OUR PREDICTIONS, WHICH WE CAN LATER CHECK FOR ACCURACY. BUT I THINK THE ROUGH PART IS THAT I DONT KNOW\n",
    "##### WHAT INFORMATION (IE DATA AND TARGETS) TO SEND WHERE AND WHEN.\n",
    "\n",
    "import operator \n",
    "def getNeighbors(trainingSet, testInstance, k):\n",
    "    distances = []\n",
    "    length = len(testInstance)-1\n",
    "    for x in range(len(trainingSet)):\n",
    "        dist = euclideanDistance(testInstance, trainingSet[x], length)\n",
    "        distances.append((trainingSet[x], dist))\n",
    "    distances.sort(key=operator.itemgetter(1))\n",
    "    neighbors = []\n",
    "    for x in range(k):\n",
    "        neighbors.append(distances[x][0])\n",
    "    return neighbors\n",
    "\n",
    "def getResponse(neighbors):\n",
    "    classVotes = {}\n",
    "    for x in range(len(neighbors)):\n",
    "        response = neighbors[x][-1]\n",
    "        if response in classVotes:\n",
    "            classVotes[response] += 1\n",
    "        else:\n",
    "            classVotes[response] = 1\n",
    "    sortedVotes = sorted(classVotes.iteritems(), key=operator.itemgetter(1), reverse=True)\n",
    "    return sortedVotes[0][0]\n",
    "\n",
    "\n",
    "# This function makes a lot of sense\n",
    "def getAccuracy(testSet, predictions):\n",
    "    correct = 0\n",
    "    for x in range(len(testSet)):\n",
    "        if testSet[x][-1] is predictions[x]:\n",
    "            correct += 1\n",
    "    return (correct/float(len(testSet))) * 100.0\n",
    "\n",
    "\n",
    "######### HERE WE CALL OUR FUNCTIONS TO ACTUALLY USE THEM\n",
    "# Generate predictions\n",
    "\n",
    "predictions=[]\n",
    "k = 3\n",
    "\n",
    "for x in range(len(testSet)):\n",
    "    neighbors = getNeighbors(trainingSet, testSet['data'], k)\n",
    "    result = getResponse(neighbors)\n",
    "    predictions.append(result)\n",
    "    print('> predicted=' + repr(result) + ', actual=' + repr(testSet['data'][-1]))\n",
    "    \n",
    "\n",
    "accuracy = getAccuracy(testSet, predictions)\n",
    "print('Accuracy: ' + repr(accuracy) + '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5) Ensemble Methods\n",
    "- Explain bagging and boosting. Clearly illustrate the difference between these methods. When would you use either one?\n",
    "- What is a decision tree? What is a random forest? Compare them and list 3 pros and cons of each?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## YOUR ANSWER HERE - YOUR MAY USE MARKDOWN, LATEX, CODE, DIAGRAMS, ETC\n",
    "\n",
    "# Bagging and boosting enhance performance. Bagging is adding more \"levels/decisions\" of equal weights whereas\n",
    "# Boosting changes the weights of certain predictors. Ex: Bagging is adding more trees in an RCF while boosting is\n",
    "# like using gradient boosting in tree classifiers\n",
    "\n",
    "# Decision tree is a single graph that uses features to decide final class\n",
    "# Forest is a bunch of trees that vote what class they think and the majority rules to classify (or a more complex merge)\n",
    "\n",
    "# Decision tree pros: easier to implement, good for combos of numerical and categorical data, easy to visualize\n",
    "# cons: can overfit if very complex but no if not,\n",
    "\n",
    "# RF pros: Usually better performance\n",
    "# cons: Can overfit, slower, harder to implement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6) PCA vs Autoencoders\n",
    "- Describe how PCA achieves dimensionality reduction. Outline the main steps of the algorithm\n",
    "- What is the importance of eigenvectors and eigenvalues in the PCA algorithm above.\n",
    "- When we compute the covariance matrix in PCA, we have to subtract the mean. Why do we do this?\n",
    "- What is Autoencoder (compare it to PCA)? Why are autoencoders better in general.\n",
    "- When is the reduced dimension of an encoder equivalent to that of a PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## YOUR ANSWER HERE - YOUR MAY USE MARKDOWN, LATEX, CODE, DIAGRAMS, ETC\n",
    "\n",
    "# PCA uses eigenvales where the projection of the data onto the eigenvector has the largest variance (spread)\n",
    "# to determine the most important distinguishing features (by selecting largest eigenvalues) to later reduce dimensionality.\n",
    "# We have to subtract mean to have the mean at 0,0. With the whitened data, you can use a formula to compute the\n",
    "# covariance matrix so it's a much simpler formula.\n",
    "# Also, from a conceptual side, your variance is defined by your variance from the mean.\n",
    "\n",
    "# autoencoders use hidden vector nodes and a bottleneck to reduce dimensionality. Performs well when you add noise\n",
    "# to dataset so it can learn what's actually important. Ignore noise more.\n",
    "# Basically autoencoders can be thought of more capable PCA where it can learn\n",
    "# non-linear manifolds (continuous, non intersecting lines)\n",
    "\n",
    "# if your autoencoder uses linear transforms (as opposed to non linear) it's basically a PCA at that point"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7) Implementation\n",
    "\n",
    "In the 1980's', Alex 'Sandy' Pentland came up with 'EigenFaces'. A novel way for facial classification using dimensionality reduction. We are going to try replicate the experiment in this question. We have loaded the face dataset for you below. Here's some steps for you: \n",
    "\n",
    "- Use PCA to reduce its dimensionality.\n",
    "- Use any algorithm to train a classifier for the dataset. You may use sklearn or pytorch. (Refer to PCA demo notebook for hints)\n",
    "- (Optional) Use autoencoders for the dimensionality reduction, compare results to PCA. Any comments/conculsions?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total dataset size:\n",
      "n_samples: 1288\n",
      "n_features: 1850\n",
      "n_classes: 7\n"
     ]
    }
   ],
   "source": [
    "# loading the faces dataset\n",
    "from sklearn.datasets import fetch_lfw_people\n",
    "\n",
    "# uncomment below to load dataset(takes ~5 mins to load data)\n",
    "lfw_people = fetch_lfw_people(min_faces_per_person=70, resize=0.4)\n",
    "\n",
    "# introspect the images arrays to find the shapes (for plotting)\n",
    "n_samples, h, w = lfw_people.images.shape\n",
    "\n",
    "# assigning features vectors\n",
    "X = lfw_people.data\n",
    "n_features = X.shape[1]\n",
    "\n",
    "# the label to predict is the id of the person\n",
    "y = lfw_people.target\n",
    "target_names = lfw_people.target_names\n",
    "n_classes = target_names.shape[0]\n",
    "\n",
    "print(\"Total dataset size:\")\n",
    "print(\"n_samples: %d\" % n_samples)\n",
    "print(\"n_features: %d\" % n_features)\n",
    "print(\"n_classes: %d\" % n_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy:  0.6356589147286822\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from matplotlib.pyplot import imread\n",
    "from skimage.transform import resize\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# plot an example image\n",
    "plt.imshow(X[1].reshape(h,w), cmap = 'gray')\n",
    "\n",
    "### insert your code here ###\n",
    "\n",
    "pca = PCA(n_components=100)\n",
    "pca.fit(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.40, random_state=42)\n",
    "\n",
    "\n",
    "RFC = RandomForestClassifier(n_estimators=100, min_samples_split=3, max_features = 50)\n",
    "RFC.fit(X_train, y_train)\n",
    "\n",
    "print(\"Training accuracy: \",accuracy_score(RFC.predict(X_test), y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus Challenge! (Optional)\n",
    "\n",
    "This will take some time. However, trust that it is a rewarding experience. There will be a prize for whoever implements it correctly!\n",
    "\n",
    "- Implement a feed forward neural network with back proprogation using stochastic gradient descent by hand. \n",
    "- Use any dataset you want and test the accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### your code below ###\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
