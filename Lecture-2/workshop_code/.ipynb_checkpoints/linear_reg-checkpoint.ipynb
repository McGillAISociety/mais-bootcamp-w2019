{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Decal, Spring 2018\n",
    "## Day 3: Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-5d21a831a125>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mstatsmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmlab\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mmlab\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.mlab as mlab\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NBA Salary Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nba_sals = pd.read_csv(\"./nbasalary.csv\", index_col = 0)\n",
    "nba_sals = nba_sals.dropna(axis=0)\n",
    "nba_sals.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_wage = nba_sals[\"lwage\"]\n",
    "wage = nba_sals[\"wage\"]\n",
    "points = nba_sals[\"points\"]\n",
    "exper = nba_sals[\"exper\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### In this section, we will compare two SLR models and see which one performs better on a validation set. \n",
    "##### Model 1: Regressing wage on points scored\n",
    "##### Model 2: Regressing wage on years of experience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.title(\"Wage vs. Points\")\n",
    "plt.xlabel(\"Points\")\n",
    "plt.ylabel(\"Wage\")\n",
    "plt.scatter(points,wage)\n",
    "myOLS_points = sm.OLS(wage,points).fit()\n",
    "plt.plot(points, myOLS_points.predict(points), color = 'red')\n",
    "\n",
    "plt.figure()\n",
    "plt.title(\"Wage vs. Experience\")\n",
    "plt.xlabel(\"Experience\")\n",
    "plt.ylabel(\"Wage\")\n",
    "plt.scatter(exper,wage)\n",
    "myOLS_exper = sm.OLS(wage,exper).fit()\n",
    "plt.plot(exper, myOLS_exper.predict(exper), color = 'red')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A little validation..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wage_train = nba_sals[\"wage\"][0:214]\n",
    "wage_valid = nba_sals[\"wage\"][214:]\n",
    "points_train = nba_sals[\"points\"][0:214]\n",
    "points_valid = nba_sals[\"points\"][214:]\n",
    "exper_train = nba_sals[\"exper\"][0:214]\n",
    "exper_valid = nba_sals[\"exper\"][214:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regression wage on points..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myOLS = sm.OLS(wage_train,points_train).fit()\n",
    "wage_hat = myOLS.predict(points_valid)\n",
    "mse = np.linalg.norm(wage_valid - wage_hat)**2/len(wage_valid)\n",
    "print(\"The MSE for the model wage~points is:\", mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regressing wage on experience..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myOLS = sm.OLS(wage_train,exper_train).fit()\n",
    "wage_hat = myOLS.predict(exper_valid)\n",
    "mse = np.linalg.norm(wage_valid - wage_hat)**2/len(wage_valid)\n",
    "print(\"The MSE for the model wage~experience is:\", mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Conclusion: points scored is a better predictor of wage than years of experience"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wage vs. Experience & Points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "exp_dat = nba_sals[[\"exper\",\"points\"]]\n",
    "exp_dat = sm.add_constant(exp_dat)\n",
    "myMLR = sm.OLS(wage, exp_dat).fit()\n",
    "myMLR.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "x1, x2 = np.meshgrid(np.linspace(exp_dat.exper.min(), exp_dat.exper.max(), 100), \n",
    "                       np.linspace(exp_dat.points.min(), exp_dat.points.max(), 100))\n",
    "\n",
    "x3 = myMLR.params[0] + myMLR.params[1] * x1 + myMLR.params[2] * x2\n",
    "\n",
    "# create matplotlib 3d axes\n",
    "fig = plt.figure(figsize=(12, 8))\n",
    "my3D = Axes3D(fig, azim=-120, elev=20)\n",
    "\n",
    "# plot hyperplane\n",
    "surf = my3D.plot_surface(x1, x2, x3, cmap=plt.cm.RdBu_r, alpha=0.5, linewidth=0.5)\n",
    "\n",
    "# plot data points\n",
    "resid = wage - myMLR.predict(exp_dat)\n",
    "my3D.scatter(exp_dat[resid >= 0].exper, exp_dat[resid >= 0].points, wage[resid >= 0], color='black', alpha=1.0, facecolor='white')\n",
    "my3D.scatter(exp_dat[resid < 0].exper, exp_dat[resid < 0].points, wage[resid < 0], color='black', alpha=1.0)\n",
    "\n",
    "# set axis labels\n",
    "my3D.set_xlabel('experience')\n",
    "my3D.set_ylabel('points')\n",
    "my3D.set_zlabel('wage')\n",
    "my3D.set_title('Regression Plane in 3D')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Polynomial Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have our data with 2 independent variables - experience and points. Let's call them $\\alpha_1$ and $\\alpha_2$ for now.\n",
    "\n",
    "So far, we have just been using the indep variables as features. So, our model was something like $$ h_{\\theta}(\\alpha) = \\theta_0 + \\theta_1*\\alpha_1 + \\theta_2*\\alpha_2 $$\n",
    "\n",
    "But what if we didn't want to have our features be just the variables? What if we wanted the features to be the square of the variables, or something more fancy? Such as this:\n",
    "\n",
    "$$ h_{\\theta}(\\alpha) = \\theta_0 + \\theta_1*\\alpha_1^2 + \\theta_2*\\alpha_2*\\alpha_1 $$\n",
    "\n",
    "In fact, this can very easily done with polynomial features. Every polynomial has a degree. The degree controls to what exponent the combined terms are. In fact, what we have been doing so far is creating polynomial features with degree d=1! Let's see what happens when d=2:\n",
    "\n",
    "$$ h_{\\theta}(\\alpha) = \\theta_0 + \\theta_1*\\alpha_1 + \\theta_2*\\alpha_2 + \\theta_3*\\alpha_1^2 + \\theta_4*\\alpha_2^2 + \\theta_5*\\alpha_1*\\alpha_2 $$\n",
    "\n",
    "What we see is we add more _cross terms_ of our features to create higher-order representations. Notice our model is still linear, though, since each $\\theta$ is still only first degree (linear)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def poly_reg(x, y, degree = 1):\n",
    "    # Add a bias term to the dataset\n",
    "    x = sm.add_constant(x)\n",
    "    \n",
    "    # Create polynomial features\n",
    "    poly_feats = PolynomialFeatures(degree)\n",
    "    x = poly_feats.fit_transform(x)\n",
    "    \n",
    "    # Split into training and validation set\n",
    "    split = int(0.8*x.shape[0])\n",
    "    x_train, x_val = x[0:split, ], x[split:, ]\n",
    "    y_train, y_val = y[:split, ], y[split:, ]\n",
    "    \n",
    "    # Fit the polynomial regression model\n",
    "    my_reg = sm.OLS(y_train, x_train).fit()\n",
    "    \n",
    "    # Make predictions\n",
    "    val_preds = my_reg.predict(x_val)\n",
    "    train_preds = my_reg.predict(x_train)\n",
    "    val_mse = mean_squared_error(y_val, val_preds)\n",
    "    train_mse = mean_squared_error(y_train, train_preds)\n",
    "    print(\"Degree:\", degree, \"\\n\", \n",
    "          \"Train MSE:\", train_mse, \"\\n\", \"Valid MSE:\", val_mse) \n",
    "    \n",
    "    return train_mse, val_mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deg_list = [1, 2, 3, 4, 5]\n",
    "t_mse, v_mse = [], []\n",
    "for deg in deg_list:\n",
    "    t, v = poly_reg(exp_dat, wage, deg)\n",
    "    t_mse.append(t)\n",
    "    v_mse.append(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot1 = plt.plot(deg_list, t_mse, '-ob', label = 'train')\n",
    "plot2 = plt.plot(deg_list, v_mse, '-or', label = 'valid')\n",
    "plt.ylabel(\"MSE\")\n",
    "plt.xlabel(\"Degree of polynomial features\")\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": false,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
