{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST handwritten digits classification with MLPs\n",
    "\n",
    "In this notebook, we'll train a multi-layer perceptron model to classify MNIST digits using **Keras** (version $\\ge$ 2 required). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Keras version: 2.2.4 backend: tensorflow\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from keras.utils import np_utils\n",
    "from keras import backend as K\n",
    "\n",
    "from distutils.version import LooseVersion as LV\n",
    "from keras import __version__\n",
    "\n",
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "print('Using Keras version:', __version__, 'backend:', K.backend())\n",
    "assert(LV(__version__) >= LV(\"2.0.0\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we are using TensorFlow as the backend, we can check whether we have GPUs available:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST data set\n",
    "\n",
    "Next we'll load the MNIST handwritten digits data set.  First time we may have to download the data, which can take a while.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://s3.amazonaws.com/img-datasets/mnist.npz\n",
      "11493376/11490434 [==============================] - 2s 0us/step\n",
      "\n",
      "MNIST data loaded: train: 60000 test: 10000\n",
      "X_train: (60000, 28, 28)\n",
      "y_train: (60000,)\n",
      "Y_train: (60000, 10)\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import mnist, fashion_mnist\n",
    "\n",
    "## MNIST:\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "## Fashion-MNIST:\n",
    "#(X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()\n",
    "\n",
    "nb_classes = 10\n",
    "\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "\n",
    "# one-hot encoding:\n",
    "Y_train = np_utils.to_categorical(y_train, nb_classes)\n",
    "Y_test = np_utils.to_categorical(y_test, nb_classes)\n",
    "\n",
    "print()\n",
    "print('MNIST data loaded: train:',len(X_train),'test:',len(X_test))\n",
    "print('X_train:', X_train.shape)\n",
    "print('y_train:', y_train.shape)\n",
    "print('Y_train:', Y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training data (`X_train`) is a 3rd-order tensor of size (60000, 28, 28), i.e. it consists of 60000 images of size 28x28 pixels. `y_train` is a 60000-dimensional vector containing the correct classes (\"0\", \"1\", ..., \"9\") for each training sample, and `Y_train` is a [one-hot](https://en.wikipedia.org/wiki/One-hot) encoding of `y_train`.\n",
    "\n",
    "Let's take a closer look. Here are the first 10 training digits (or fashion items for Fashion-MNIST):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training sample 0 : class: 5 , one-hot encoded: [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      "Training sample 1 : class: 0 , one-hot encoded: [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Training sample 2 : class: 4 , one-hot encoded: [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      "Training sample 3 : class: 1 , one-hot encoded: [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Training sample 4 : class: 9 , one-hot encoded: [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      "Training sample 5 : class: 2 , one-hot encoded: [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Training sample 6 : class: 1 , one-hot encoded: [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Training sample 7 : class: 3 , one-hot encoded: [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      "Training sample 8 : class: 1 , one-hot encoded: [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Training sample 9 : class: 4 , one-hot encoded: [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x72 with 10 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pltsize=1\n",
    "plt.figure(figsize=(10*pltsize, pltsize))\n",
    "\n",
    "for i in range(10):\n",
    "    plt.subplot(1,10,i+1)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(X_train[i,:,:], cmap=\"gray\")\n",
    "    plt.title('Class: '+str(y_train[i]))\n",
    "    print('Training sample',i,': class:',y_train[i], ', one-hot encoded:', Y_train[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear model\n",
    "\n",
    "### Initialization\n",
    "\n",
    "Let's begin with a simple linear model.  We first initialize the model with `Sequential()`.  Then we add a `Dense` layer that has 28*28=784 input nodes (one for each pixel in the input image) and 10 output nodes. The `Dense` layer connects each input to each output with some weight parameter. \n",
    "\n",
    "Finally, we select *categorical crossentropy* as the loss function, select [*stochastic gradient descent*](https://keras.io/optimizers/#sgd) as the optimizer, add *accuracy* to the list of metrics to be evaluated, and `compile()` the model. Note there are [several different options](https://keras.io/optimizers/) for the optimizer in Keras that we could use instead of *sgd*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linmodel = Sequential()\n",
    "linmodel.add(Dense(units=10, input_dim=28*28, activation='softmax'))\n",
    "\n",
    "linmodel.compile(loss='categorical_crossentropy', \n",
    "                 optimizer='sgd', \n",
    "                 metrics=['accuracy'])\n",
    "print(linmodel.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The summary shows that there are 7850 parameters in our model, as the weight matrix is of size 785x10 (not 784, as there's an additional bias term).\n",
    "\n",
    "We can also draw a fancier graph of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVG(model_to_dot(linmodel, show_shapes=True).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning\n",
    "\n",
    "Now we are ready to train our first model.  An *epoch* means one pass through the whole training data. \n",
    "\n",
    "The `reshape()` function flattens our 28x28 images into vectors of length 784.  (This means we are not using any information about the spatial neighborhood relations of pixels.  This setup is known as the *permutation invariant MNIST*.)  \n",
    "\n",
    "You can run code below multiple times and it will continue the training process from where it left off.  If you want to start from scratch, re-initialize the model using the code a few cells ago. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "epochs = 10 # one epoch takes about 3 seconds\n",
    "\n",
    "linhistory = linmodel.fit(X_train.reshape((-1,28*28)), \n",
    "                          Y_train, \n",
    "                          epochs=epochs, \n",
    "                          batch_size=32,\n",
    "                          verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now see how the training progressed. \n",
    "\n",
    "* *Loss* is a function of the difference of the network output and the target values.  We are minimizing the loss function during training so it should decrease over time.\n",
    "* *Accuracy* is the classification accuracy for the training data.  It gives some indication of the real accuracy of the model but cannot be fully trusted, as it may have overfitted and just memorizes the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5,3))\n",
    "plt.plot(linhistory.epoch,linhistory.history['loss'])\n",
    "plt.title('loss')\n",
    "\n",
    "plt.figure(figsize=(5,3))\n",
    "plt.plot(linhistory.epoch,linhistory.history['acc'])\n",
    "plt.title('accuracy');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference\n",
    "\n",
    "For a better measure of the quality of the model, let's see the model accuracy for the test data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linscores = linmodel.evaluate(X_test.reshape((-1,28*28)), \n",
    "                              Y_test, \n",
    "                              verbose=2)\n",
    "print(\"%s: %.2f%%\" % (linmodel.metrics_names[1], linscores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now take a closer look on the results.\n",
    "\n",
    "Let's define a helper function to show the failure cases of our classifier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_failures(predictions, trueclass=None, predictedclass=None, maxtoshow=10):\n",
    "    rounded = np.argmax(predictions, axis=1)\n",
    "    errors = rounded!=y_test\n",
    "    print('Showing max', maxtoshow, 'first failures. '\n",
    "          'The predicted class is shown first and the correct class in parenthesis.')\n",
    "    ii = 0\n",
    "    plt.figure(figsize=(maxtoshow, 1))\n",
    "    for i in range(X_test.shape[0]):\n",
    "        if ii>=maxtoshow:\n",
    "            break\n",
    "        if errors[i]:\n",
    "            if trueclass is not None and y_test[i] != trueclass:\n",
    "                continue\n",
    "            if predictedclass is not None and rounded[i] != predictedclass:\n",
    "                continue\n",
    "            plt.subplot(1, maxtoshow, ii+1)\n",
    "            plt.axis('off')\n",
    "            plt.imshow(X_test[i,:,:], cmap=\"gray\")\n",
    "            plt.title(\"%d (%d)\" % (rounded[i], y_test[i]))\n",
    "            ii = ii + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the first 10 test digits the linear model classified to a wrong class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linpredictions = linmodel.predict(X_test.reshape((-1,28*28)))\n",
    "\n",
    "show_failures(linpredictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-layer perceptron (MLP) network\n",
    "\n",
    "### Activation functions\n",
    "\n",
    "Let's start by plotting some common activation functions for neural networks. `'relu'` stands for rectified linear unit, $y=\\max(0,x)$, a very simple non-linearity we will be using in our MLP network below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(-4,4,.01)\n",
    "plt.figure()\n",
    "plt.plot(x, np.maximum(x,0), label='relu')\n",
    "plt.plot(x, 1/(1+np.exp(-x)), label='sigmoid')\n",
    "plt.plot(x, np.tanh(x), label='tanh')\n",
    "plt.axis([-4, 4, -1.1, 1.5])\n",
    "plt.title('Activation functions')\n",
    "plt.legend(loc='best');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialization\n",
    "\n",
    "Let's now create a more complex MLP model that has multiple layers, non-linear activation functions, and dropout layers.  `Dropout()` randomly sets a fraction of inputs to zero during training, which is one approach to regularization and can sometimes help to prevent overfitting.\n",
    "\n",
    "There are two options below, a simple and a bit more complex model.  Select either one.\n",
    "\n",
    "The output of the last layer needs to be a softmaxed 10-dimensional vector to match the groundtruth (`Y_train`). \n",
    "\n",
    "Finally, we again `compile()` the model, this time using [*Adam*](https://keras.io/optimizers/#adam) as the optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model initialization:\n",
    "model = Sequential()\n",
    "\n",
    "# A simple model:\n",
    "model.add(Dense(units=20, input_dim=28*28))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "# A bit more complex model:\n",
    "#model.add(Dense(units=50, input_dim=28*28))\n",
    "#model.add(Activation('relu'))\n",
    "#model.add(Dropout(0.2))\n",
    "\n",
    "#model.add(Dense(units=50))\n",
    "#model.add(Activation('relu'))\n",
    "#model.add(Dropout(0.2))\n",
    "\n",
    "# The last layer needs to be like this:\n",
    "model.add(Dense(units=10, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', \n",
    "              optimizer='adam', \n",
    "              metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVG(model_to_dot(model, show_shapes=True).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "epochs = 10 # one epoch with simple model takes about 4 seconds\n",
    "\n",
    "history = model.fit(X_train.reshape((-1,28*28)), \n",
    "                    Y_train, \n",
    "                    epochs=epochs, \n",
    "                    batch_size=32,\n",
    "                    verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5,3))\n",
    "plt.plot(history.epoch,history.history['loss'])\n",
    "plt.title('loss')\n",
    "\n",
    "plt.figure(figsize=(5,3))\n",
    "plt.plot(history.epoch,history.history['acc'])\n",
    "plt.title('accuracy');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference\n",
    "\n",
    "Accuracy for test data.  The model should be somewhat better than the linear model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "scores = model.evaluate(X_test.reshape((-1,28*28)), Y_test, verbose=2)\n",
    "print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can again take a closer look on the results, using the `show_failures()` function defined earlier.\n",
    "\n",
    "Here are the first 10 test digits the MLP classified to a wrong class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(X_test.reshape((-1,28*28)))\n",
    "\n",
    "show_failures(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use `show_failures()` to inspect failures in more detail. For example, here are failures in which the true class was \"6\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_failures(predictions, trueclass=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also compute the confusion matrix to see which digits get mixed the most, and look at classification accuracies separately for each class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import pandas as pd\n",
    "\n",
    "print('Confusion matrix (rows: true classes; columns: predicted classes):'); print()\n",
    "cm=confusion_matrix(y_test, np.argmax(predictions, axis=1), labels=list(range(10)))\n",
    "\n",
    "df_cm = pd.DataFrame(cm)\n",
    "plt.figure(figsize = (10,7))\n",
    "sns.heatmap(df_cm, annot=True, cmap=\"YlGnBu\")\n",
    "\n",
    "print('Classification accuracy for each class:'); print()\n",
    "for i,j in enumerate(cm.diagonal()/cm.sum(axis=1)): print(\"%d: %.4f\" % (i,j))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model tuning\n",
    "\n",
    "Modify the MLP model.  Try to improve the classification accuracy, or experiment with the effects of different parameters.  If you are interested in the state-of-the-art performance on permutation invariant MNIST, see e.g. this [recent paper](https://arxiv.org/abs/1507.02672) by Aalto University / The Curious AI Company researchers.\n",
    "\n",
    "You can also consult the Keras documentation at https://keras.io/.  For example, the Dense, Activation, and Dropout layers are described at https://keras.io/layers/core/."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "*Run this notebook in Google Colaboratory using [this link](https://colab.research.google.com/github/csc-training/intro-to-dl/blob/master/day1/keras-mnist-mlp.ipynb).*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": false,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
