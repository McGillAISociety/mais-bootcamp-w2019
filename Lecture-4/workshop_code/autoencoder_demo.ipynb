{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from matplotlib.pyplot import imread\n",
    "from sklearn.decomposition import PCA\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load MNIST data and visualize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_train, mnist_val = {}, {}\n",
    "for digit in range(10):\n",
    "    arr = np.load('data/mnist/{0}.npy'.format(digit))\n",
    "    num_train = int(0.9 * arr.shape[0])\n",
    "    mnist_train[digit] = arr[:num_train]\n",
    "    mnist_val[digit] = arr[num_train:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "selected_images = []\n",
    "for digit in mnist_train:\n",
    "    image_idx = np.random.choice(len(mnist_train[digit]))\n",
    "    image = mnist_train[digit][image_idx]\n",
    "    selected_images.append(image)\n",
    "\n",
    "plt.figure(figsize=(20, 2))\n",
    "plt.imshow(np.concatenate(selected_images, axis=1), cmap='Greys')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform PCA on MNIST onto two components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.concatenate([mnist_train[digit].reshape(-1, 28 ** 2) for digit in mnist_train], axis=0)\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "pca.fit(X)\n",
    "\n",
    "plt.figure(figsize=(14, 14))\n",
    "for digit in mnist_train:\n",
    "    transformed = pca.transform(mnist_train[digit].reshape(-1, 28 ** 2))\n",
    "    plt.scatter(transformed[:, 0], transformed[:, 1], s=2, label=digit)\n",
    "\n",
    "plt.legend(markerscale=8)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "colors = plt.rcParams['axes.prop_cycle'].by_key()['color']\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(14, 7))\n",
    "for digit in range(5):\n",
    "    transformed = pca.transform(mnist_train[digit].reshape(-1, 28 ** 2))\n",
    "    ax[0].scatter(transformed[:, 0], transformed[:, 1], s=1, label=digit, c=colors[digit])\n",
    "for digit in range(5, 10):\n",
    "    transformed = pca.transform(mnist_train[digit].reshape(-1, 28 ** 2))\n",
    "    ax[1].scatter(transformed[:, 0], transformed[:, 1], s=1, label=digit, c=colors[digit])\n",
    "    \n",
    "ax[0].legend(markerscale=8)\n",
    "ax[1].legend(markerscale=8)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We see that individual clusters of data are visible when we use PCA! That means that important information about the data is preserved when we compress using PCA. However, some of the clusters overlap, such as the '8' and '3' clusters. Let's do better with autoencoders."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create our autoencoder with PyTorch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we'll define the structure of our autoencoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, sizes):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        encoder_layers = []\n",
    "        for inp, out in zip(sizes[:-2], sizes[1:-1]):\n",
    "            encoder_layers.append(nn.Linear(inp, out))\n",
    "            encoder_layers.append(nn.ReLU())\n",
    "        encoder_layers.append(nn.Linear(sizes[-2], sizes[-1]))\n",
    "            \n",
    "        self.encoder = nn.Sequential(*encoder_layers)\n",
    "        \n",
    "        \n",
    "        decoder_layers = []\n",
    "        for inp, out in zip(sizes[::-1][:-2], sizes[::-1][1:-1]):\n",
    "            decoder_layers.append(nn.Linear(inp, out))\n",
    "            decoder_layers.append(nn.ReLU())\n",
    "        decoder_layers.append(nn.Linear(sizes[::-1][-2], sizes[::-1][-1]))\n",
    "        decoder_layers.append(nn.Sigmoid())\n",
    "            \n",
    "        self.decoder = nn.Sequential(*decoder_layers)\n",
    "            \n",
    "    def forward(self, inp):\n",
    "        return self.decoder(self.encoder(inp))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we'll create and train our autoencoder on MNIST with stochastic gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model = Autoencoder([\n",
    "    28 ** 2,\n",
    "    512,\n",
    "    128,\n",
    "    2\n",
    "])\n",
    "print(model)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "X_train = np.concatenate([mnist_train[digit].reshape(-1, 28 ** 2) for digit in mnist_train], axis=0)\n",
    "X_train = torch.from_numpy(X_train)\n",
    "np.random.shuffle(X_train)\n",
    "\n",
    "X_val = np.concatenate([mnist_val[digit].reshape(-1, 28 ** 2) for digit in mnist_val], axis=0)\n",
    "X_val = torch.from_numpy(X_val)\n",
    "np.random.shuffle(X_val)\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "for epoch in range(10):\n",
    "    for i in range(len(X_train) // batch_size):\n",
    "        \n",
    "        inp = X_train[batch_size * i : batch_size * i + batch_size]\n",
    "        output = model(inp)\n",
    "        train_loss = torch.mean((inp - output) ** 2)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    train_loss = torch.mean((X_train - model(X_train)) ** 2).item()\n",
    "    validation_loss = torch.mean((X_val - model(X_val)) ** 2).item()\n",
    "    print(\"Epoch {0}: Train loss = {1}, validation loss = {2}\".format(epoch, train_loss, validation_loss))\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize the compression that the autoencoder has learned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14, 14))\n",
    "for digit in mnist_val:\n",
    "    inp = torch.tensor(mnist_val[digit].reshape(-1, 28 ** 2))\n",
    "    transformed = model.encoder(inp).detach().numpy()\n",
    "    plt.scatter(transformed[:, 0], transformed[:, 1], s=2, label=digit)\n",
    "\n",
    "plt.legend(markerscale=8)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
